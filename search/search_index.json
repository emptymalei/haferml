{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation for HAFER ML \u00a4 The HomemAde FramEwoRk for Machine Learning (HAFER ML). Hafer is oat in German. Install \u00a4 pip install haferml This will leave out many dependencies. To install haferml together with all the requirements, pip install \"haferml[all]\" The extras options: all : everything aws : required if one needs AWS docs : required to build the docs The Idea \u00a4 The Hafer ML framework is designed to be a minimal framework for data scientists. Hafer ML uses config files and saves the artifacts at every step for reproducibility. Toolkit \u00a4 The essential tools are Python, Docker, AWS: such as S3, ECR and ECS.","title":"Home"},{"location":"#documentation-for-hafer-ml","text":"The HomemAde FramEwoRk for Machine Learning (HAFER ML). Hafer is oat in German.","title":"Documentation for HAFER ML"},{"location":"#install","text":"pip install haferml This will leave out many dependencies. To install haferml together with all the requirements, pip install \"haferml[all]\" The extras options: all : everything aws : required if one needs AWS docs : required to build the docs","title":"Install"},{"location":"#the-idea","text":"The Hafer ML framework is designed to be a minimal framework for data scientists. Hafer ML uses config files and saves the artifacts at every step for reproducibility.","title":"The Idea"},{"location":"#toolkit","text":"The essential tools are Python, Docker, AWS: such as S3, ECR and ECS.","title":"Toolkit"},{"location":"changelog/","text":"HAFER ML Changelog \u00a4 2021-05-20, 0.0.12 \u00a4 We have added two new decorators for convenience. order In the previous version, we had attributes . Most of the time, we do not care about which attribute name that we used if we just need to order the member functions. For this reason, we created this new decorator order and simply use @order(1) and we are done. with_transforms If one would like to write a customized transform class, it takes a lot of effort in the previous version of the package. Now we can simply apply the decorator @with_transforms and the member function can access the ordered transforms using self.transforms . 2021-05-09, 0.0.11 \u00a4 Added config generation tool. 2021-05-09, 0.0.10 \u00a4 Change BasePreProcessor member function name and make it more flexible.","title":"Changelog"},{"location":"changelog/#hafer-ml-changelog","text":"","title":"HAFER ML Changelog"},{"location":"changelog/#2021-05-20-0012","text":"We have added two new decorators for convenience. order In the previous version, we had attributes . Most of the time, we do not care about which attribute name that we used if we just need to order the member functions. For this reason, we created this new decorator order and simply use @order(1) and we are done. with_transforms If one would like to write a customized transform class, it takes a lot of effort in the previous version of the package. Now we can simply apply the decorator @with_transforms and the member function can access the ordered transforms using self.transforms .","title":"2021-05-20, 0.0.12"},{"location":"changelog/#2021-05-09-0011","text":"Added config generation tool.","title":"2021-05-09, 0.0.11"},{"location":"changelog/#2021-05-09-0010","text":"Change BasePreProcessor member function name and make it more flexible.","title":"2021-05-09, 0.0.10"},{"location":"references/","text":"References \u00a4 In this section, we provide the references for the Hafer ML codebase. For more details of how to use the package, please refer to tutorials . Warning This is still a WIP.","title":"Introduction"},{"location":"references/#references","text":"In this section, we provide the references for the Hafer ML codebase. For more details of how to use the package, please refer to tutorials . Warning This is still a WIP.","title":"References"},{"location":"references/blend/","text":"Blend \u00a4 The blender machine to mix everything together.","title":"Index"},{"location":"references/blend/#blend","text":"The blender machine to mix everything together.","title":"Blend"},{"location":"references/blend/config/","text":"Config \u00a4 Utilities to deal with the configuration files. Config \u00a4 Config makes it easy to load and use config files. A Config object will load the config file and enhance the paths, e.g., prepend the base folder, if needed. The config object can also be used like a dictionary or even more convinently using a list as the path to the value. conf = Config(file_path=\"test.json\", base_folder=\"/tmp\") conf[[\"etl\", \"raw\"]] Parameters: Name Type Description Default file_path str path to the config file. If the path is relative path, please specify the base folder. required base_folder str, optional the base folder for our working directory, defaults to None required _enhance_local_paths ( config , base_folder = None ) private staticmethod \u00a4 Appends base_folder to the local paths in the configs and also the file path if name key is present. Parameters: Name Type Description Default config dict dictionary of configuration. required base_folder str, optional base folder of all the artifacts, defaults to None None Source code in haferml/blend/config.py @staticmethod def _enhance_local_paths ( config , base_folder = None ): \"\"\" Appends base_folder to the local paths in the configs and also the file path if name key is present. :param config: dictionary of configuration. :type config: dict :param base_folder: base folder of all the artifacts, defaults to None :type base_folder: str, optional \"\"\" all_paths = _get_all_paths_in_dict ( config ) for p in all_paths : if p [ - 1 ] == \"local\" : p_local_value = get_config ( config , p ) p_local_parent_path = p [: - 1 ] if base_folder is not None : p_local_value = os . path . join ( base_folder , p_local_value ) _update_dict_recursively ( config , p_local_parent_path + [ \"local_absolute\" ], p_local_value ) p_local_parent_value = get_config ( config , p_local_parent_path ) if \"name\" in p_local_parent_value : p_name_value = p_local_parent_value [ \"name\" ] p_name_value = os . path . join ( p_local_value , p_name_value ) _update_dict_recursively ( config , p_local_parent_path + [ \"name_absolute\" ], p_name_value ) get ( self , path ) \u00a4 Retrieve config for a given path down in the configs. conf = Config(file_path=\"test.json\", base_folder=\"/tmp\") conf.get([\"etl\", \"raw\"]) Parameters: Name Type Description Default path list path to the specific configurations required Returns: Type Description configuration of for the specific path Source code in haferml/blend/config.py def get ( self , path ): \"\"\" Retrieve config for a given path down in the configs. ``` conf = Config(file_path=\"test.json\", base_folder=\"/tmp\") conf.get([\"etl\", \"raw\"]) ``` :param path: path to the specific configurations :type path: list :return: configuration of for the specific path \"\"\" config = get_config ( self . config , path ) return config construct_paths ( config , base_folder ) \u00a4 construct_paths reconstructs the path based on base folder. The local key in config will be used. Typically, the config shall be something like config = { \"local\": \"gauss/data\", \"name\": \"my_data.parquet\" } If the base folder is base_folder=/tmp , the config will become config = { \"local\": \"/tmp/gauss/data\", \"name\": \"my_data.parquet\", \"file_path\": \"/tmp/gauss/data/my_data.parquet\" } Parameters: Name Type Description Default config dict the config dictionary that contains a local key required base_folder str base folder that will be prepended to the path in config[\"local\"] required Source code in haferml/blend/config.py def construct_paths ( config , base_folder ): \"\"\" construct_paths reconstructs the path based on base folder. The `local` key in `config` will be used. Typically, the config shall be something like ``` config = { \"local\": \"gauss/data\", \"name\": \"my_data.parquet\" } ``` If the base folder is `base_folder=/tmp`, the config will become ``` config = { \"local\": \"/tmp/gauss/data\", \"name\": \"my_data.parquet\", \"file_path\": \"/tmp/gauss/data/my_data.parquet\" } ``` :param dict config: the config dictionary that contains a `local` key :param str base_folder: base folder that will be prepended to the path in `config[\"local\"]` \"\"\" if not config . get ( \"local\" ): logger . warning ( f \" { config } does not contain local key \" ) return config config_recon = {} config_local = config [ \"local\" ] config_name = config . get ( \"name\" ) config_local = os . path . join ( base_folder , config_local ) config_recon [ \"local\" ] = config_local if config_name : config_local_full = os . path . join ( config_local , config_name ) config_recon [ \"file_path\" ] = config_local_full return { ** config , ** config_recon } get_config ( configs , path ) \u00a4 Get value of the configs under specified path >>> get_config({'etl':{'raw':{'local':'data/raw', 'remote': 's3://haferml-tutorials/rideindego/marshall/data/raw'}}},['etl','raw']) {'local':'data/raw', 'remote': 's3://haferml-tutorials/rideindego/marshall/data/raw'} Parameters: Name Type Description Default configs dict input dictionary required path list path to the value to be obtained required Source code in haferml/blend/config.py def get_config ( configs , path ): \"\"\" Get value of the configs under specified path ``` >>> get_config({'etl':{'raw':{'local':'data/raw', 'remote': 's3://haferml-tutorials/rideindego/marshall/data/raw'}}},['etl','raw']) {'local':'data/raw', 'remote': 's3://haferml-tutorials/rideindego/marshall/data/raw'} ``` :param dict configs: input dictionary :param list path: path to the value to be obtained \"\"\" # Construct the path if not isinstance ( path , ( list , tuple )): logger . warning ( f \"path is not list nor tuple, converting to list: { path } \" ) path = [ path ] # Find the values res = configs . copy () for p in path : res = res [ p ] return res load_config ( config_path , base_folder = None ) \u00a4 load_config loads the config files of the project from a path and generate a dictionary. If no base_folder is not specified, the path will be treated as it is. For example, config_path=a/b/c.json will be a file relative to the python work directory; while config_path=/a/b/c.json will be the absolute path. Parameters: Name Type Description Default config_path str path to the config file required base_folder str the base folder of the whole project None Source code in haferml/blend/config.py def load_config ( config_path , base_folder = None ): \"\"\" load_config loads the config files of the project from a path and generate a dictionary. If no `base_folder` is not specified, the path will be treated as it is. For example, `config_path=a/b/c.json` will be a file relative to the python work directory; while `config_path=/a/b/c.json` will be the absolute path. :param str config_path: path to the config file :param str base_folder: the base folder of the whole project :param base_folder: str, optional \"\"\" if config_path is None : raise Exception ( f \"config_path has not been specified...\" ) if base_folder is not None : config_path = os . path . join ( base_folder , config_path ) if not os . path . exists ( config_path ): raise Exception ( f \"config file path { config_path } does not exist! Beware of the relative path.\" ) logger . debug ( f \"Loading config from { config_path } \" ) with open ( config_path , \"r\" ) as fp : config = json . load ( fp ) if not config : logger . warning ( f \"The config is empty: { config } \" ) return config","title":"blend.config"},{"location":"references/blend/config/#config","text":"Utilities to deal with the configuration files.","title":"Config"},{"location":"references/blend/config/#haferml.blend.config.Config","text":"Config makes it easy to load and use config files. A Config object will load the config file and enhance the paths, e.g., prepend the base folder, if needed. The config object can also be used like a dictionary or even more convinently using a list as the path to the value. conf = Config(file_path=\"test.json\", base_folder=\"/tmp\") conf[[\"etl\", \"raw\"]] Parameters: Name Type Description Default file_path str path to the config file. If the path is relative path, please specify the base folder. required base_folder str, optional the base folder for our working directory, defaults to None required","title":"Config"},{"location":"references/blend/config/#haferml.blend.config.Config._enhance_local_paths","text":"Appends base_folder to the local paths in the configs and also the file path if name key is present. Parameters: Name Type Description Default config dict dictionary of configuration. required base_folder str, optional base folder of all the artifacts, defaults to None None Source code in haferml/blend/config.py @staticmethod def _enhance_local_paths ( config , base_folder = None ): \"\"\" Appends base_folder to the local paths in the configs and also the file path if name key is present. :param config: dictionary of configuration. :type config: dict :param base_folder: base folder of all the artifacts, defaults to None :type base_folder: str, optional \"\"\" all_paths = _get_all_paths_in_dict ( config ) for p in all_paths : if p [ - 1 ] == \"local\" : p_local_value = get_config ( config , p ) p_local_parent_path = p [: - 1 ] if base_folder is not None : p_local_value = os . path . join ( base_folder , p_local_value ) _update_dict_recursively ( config , p_local_parent_path + [ \"local_absolute\" ], p_local_value ) p_local_parent_value = get_config ( config , p_local_parent_path ) if \"name\" in p_local_parent_value : p_name_value = p_local_parent_value [ \"name\" ] p_name_value = os . path . join ( p_local_value , p_name_value ) _update_dict_recursively ( config , p_local_parent_path + [ \"name_absolute\" ], p_name_value )","title":"_enhance_local_paths()"},{"location":"references/blend/config/#haferml.blend.config.Config.get","text":"Retrieve config for a given path down in the configs. conf = Config(file_path=\"test.json\", base_folder=\"/tmp\") conf.get([\"etl\", \"raw\"]) Parameters: Name Type Description Default path list path to the specific configurations required Returns: Type Description configuration of for the specific path Source code in haferml/blend/config.py def get ( self , path ): \"\"\" Retrieve config for a given path down in the configs. ``` conf = Config(file_path=\"test.json\", base_folder=\"/tmp\") conf.get([\"etl\", \"raw\"]) ``` :param path: path to the specific configurations :type path: list :return: configuration of for the specific path \"\"\" config = get_config ( self . config , path ) return config","title":"get()"},{"location":"references/blend/config/#haferml.blend.config.construct_paths","text":"construct_paths reconstructs the path based on base folder. The local key in config will be used. Typically, the config shall be something like config = { \"local\": \"gauss/data\", \"name\": \"my_data.parquet\" } If the base folder is base_folder=/tmp , the config will become config = { \"local\": \"/tmp/gauss/data\", \"name\": \"my_data.parquet\", \"file_path\": \"/tmp/gauss/data/my_data.parquet\" } Parameters: Name Type Description Default config dict the config dictionary that contains a local key required base_folder str base folder that will be prepended to the path in config[\"local\"] required Source code in haferml/blend/config.py def construct_paths ( config , base_folder ): \"\"\" construct_paths reconstructs the path based on base folder. The `local` key in `config` will be used. Typically, the config shall be something like ``` config = { \"local\": \"gauss/data\", \"name\": \"my_data.parquet\" } ``` If the base folder is `base_folder=/tmp`, the config will become ``` config = { \"local\": \"/tmp/gauss/data\", \"name\": \"my_data.parquet\", \"file_path\": \"/tmp/gauss/data/my_data.parquet\" } ``` :param dict config: the config dictionary that contains a `local` key :param str base_folder: base folder that will be prepended to the path in `config[\"local\"]` \"\"\" if not config . get ( \"local\" ): logger . warning ( f \" { config } does not contain local key \" ) return config config_recon = {} config_local = config [ \"local\" ] config_name = config . get ( \"name\" ) config_local = os . path . join ( base_folder , config_local ) config_recon [ \"local\" ] = config_local if config_name : config_local_full = os . path . join ( config_local , config_name ) config_recon [ \"file_path\" ] = config_local_full return { ** config , ** config_recon }","title":"construct_paths()"},{"location":"references/blend/config/#haferml.blend.config.get_config","text":"Get value of the configs under specified path >>> get_config({'etl':{'raw':{'local':'data/raw', 'remote': 's3://haferml-tutorials/rideindego/marshall/data/raw'}}},['etl','raw']) {'local':'data/raw', 'remote': 's3://haferml-tutorials/rideindego/marshall/data/raw'} Parameters: Name Type Description Default configs dict input dictionary required path list path to the value to be obtained required Source code in haferml/blend/config.py def get_config ( configs , path ): \"\"\" Get value of the configs under specified path ``` >>> get_config({'etl':{'raw':{'local':'data/raw', 'remote': 's3://haferml-tutorials/rideindego/marshall/data/raw'}}},['etl','raw']) {'local':'data/raw', 'remote': 's3://haferml-tutorials/rideindego/marshall/data/raw'} ``` :param dict configs: input dictionary :param list path: path to the value to be obtained \"\"\" # Construct the path if not isinstance ( path , ( list , tuple )): logger . warning ( f \"path is not list nor tuple, converting to list: { path } \" ) path = [ path ] # Find the values res = configs . copy () for p in path : res = res [ p ] return res","title":"get_config()"},{"location":"references/blend/config/#haferml.blend.config.load_config","text":"load_config loads the config files of the project from a path and generate a dictionary. If no base_folder is not specified, the path will be treated as it is. For example, config_path=a/b/c.json will be a file relative to the python work directory; while config_path=/a/b/c.json will be the absolute path. Parameters: Name Type Description Default config_path str path to the config file required base_folder str the base folder of the whole project None Source code in haferml/blend/config.py def load_config ( config_path , base_folder = None ): \"\"\" load_config loads the config files of the project from a path and generate a dictionary. If no `base_folder` is not specified, the path will be treated as it is. For example, `config_path=a/b/c.json` will be a file relative to the python work directory; while `config_path=/a/b/c.json` will be the absolute path. :param str config_path: path to the config file :param str base_folder: the base folder of the whole project :param base_folder: str, optional \"\"\" if config_path is None : raise Exception ( f \"config_path has not been specified...\" ) if base_folder is not None : config_path = os . path . join ( base_folder , config_path ) if not os . path . exists ( config_path ): raise Exception ( f \"config file path { config_path } does not exist! Beware of the relative path.\" ) logger . debug ( f \"Loading config from { config_path } \" ) with open ( config_path , \"r\" ) as fp : config = json . load ( fp ) if not config : logger . warning ( f \"The config is empty: { config } \" ) return config","title":"load_config()"},{"location":"references/command/","text":"Command Line Tool \u00a4 haferml.command module contains the command toolkits. To find all the commands, use haferml --help . Creating Config File Templates \u00a4 To create a config file template for your project, use the command haferml config path/to/your/config/file.json . For more information, run the command haferml config --help .","title":"Commandline Tool"},{"location":"references/command/#command-line-tool","text":"haferml.command module contains the command toolkits. To find all the commands, use haferml --help .","title":"Command Line Tool"},{"location":"references/command/#creating-config-file-templates","text":"To create a config file template for your project, use the command haferml config path/to/your/config/file.json . For more information, run the command haferml config --help .","title":"Creating Config File Templates"},{"location":"references/data/","text":"Data \u00a4 A few utilities for data related tasks. This module is not intended to build a Swiss army knife module for data manipulations. In fact, only utilities that are necessary for the pipelines are included.","title":"Index"},{"location":"references/data/#data","text":"A few utilities for data related tasks. This module is not intended to build a Swiss army knife module for data manipulations. In fact, only utilities that are necessary for the pipelines are included.","title":"Data"},{"location":"references/data/wrangle/","text":"Data - Wrangling \u00a4 The utilities included are for some essential data wrangling in the pipelines.","title":"Index"},{"location":"references/data/wrangle/#data-wrangling","text":"The utilities included are for some essential data wrangling in the pipelines.","title":"Data - Wrangling"},{"location":"references/data/wrangle/datetime/","text":"Data - Wrangling - Datetime \u00a4 convert_to_datetime ( input_date , dayfirst = None , input_tz = None , output_tz = None ) \u00a4 Convert input to datetime object. This is the last effort of converting input to datetime. The order of instance check is 1. datetime.datetime 2. str 3. float or int >>> handle_strange_dates(1531323212311) datetime(2018, 7, 11, 17, 33, 32, 311000) >>> handle_strange_dates(datetime(2085,1,1)) datetime(2050, 1, 1) Parameters: Name Type Description Default input_date input data of any possible format required input_tz input timezone, defaults to utc None output_tz output timezone, defaults to utc None Returns: Type Description datetime.datetime converted datetime format Source code in haferml/data/wrangle/datetime.py def convert_to_datetime ( input_date , dayfirst = None , input_tz = None , output_tz = None ): \"\"\" Convert input to *datetime* object. This is the last effort of converting input to datetime. The order of instance check is 1. datetime.datetime 2. str 3. float or int ``` >>> handle_strange_dates(1531323212311) datetime(2018, 7, 11, 17, 33, 32, 311000) >>> handle_strange_dates(datetime(2085,1,1)) datetime(2050, 1, 1) ``` :param input_date: input data of any possible format :param input_tz: input timezone, defaults to utc :param output_tz: output timezone, defaults to utc :return: converted datetime format :rtype: datetime.datetime \"\"\" if dayfirst is None : dayfirst = True if input_tz is None : input_tz = datetime . timezone . utc if output_tz is None : output_tz = datetime . timezone . utc res = None if isinstance ( input_date , datetime . datetime ): res = input_date if input_tz : res = res . replace ( tzinfo = input_tz ) if output_tz : res = res . astimezone ( output_tz ) elif isinstance ( input_date , str ): try : res = dateutil . parser . parse ( input_date , dayfirst = dayfirst ) if input_tz : res = res . replace ( tzinfo = input_tz ) if output_tz : res = res . astimezone ( output_tz ) except : logger . warning ( f \"Could not convert { input_date } to datetime!\" ) pass elif isinstance ( input_date , ( float , int )): try : res = datetime . datetime . utcfromtimestamp ( input_date / 1000 ) if input_tz : res = res . replace ( tzinfo = input_tz ) if output_tz : res = res . astimezone ( output_tz ) except : logger . warning ( f \"Could not convert { input_date } to datetime!\" ) pass else : raise Exception ( \"Could not convert {} to datetime: type {} is not handled\" . format ( input_date , type ( input_date ) ) ) return res date_range_has_weekday ( dt_start , dt_end ) \u00a4 date_range_has_weekday decides if the given date range contains weekday Parameters: Name Type Description Default dt_start datetime of the start of date range required dt_end datetime of the end of date range required Source code in haferml/data/wrangle/datetime.py def date_range_has_weekday ( dt_start , dt_end ): \"\"\" date_range_has_weekday decides if the given date range contains weekday :param dt_start: datetime of the start of date range :param dt_end: datetime of the end of date range \"\"\" res = [] if pd . isnull ( dt_start ) or pd . isnull ( dt_end ): logger . warning ( f \"date start end not specified: { dt_start } , { dt_end } \" ) return None if isinstance ( dt_start , str ): dt_start = pd . to_datetime ( dt_start ) if isinstance ( dt_end , str ): dt_end = pd . to_datetime ( dt_end ) for dt in pd . date_range ( dt_start , dt_end ): if dt . weekday () < 5 : res . append ( True ) else : res . append ( False ) return True in res unpack_datetime ( data ) \u00a4 unpack_datetime converts datetime (string) to a dict of useful date information Source code in haferml/data/wrangle/datetime.py def unpack_datetime ( data ): \"\"\" unpack_datetime converts datetime (string) to a dict of useful date information \"\"\" res = {} dt = convert_to_datetime ( data , dayfirst = False ) if dt : try : res [ \"year\" ] = dt . year except Exception as e : logger . error ( f \"Could not find year for { dt } (raw: { data } )\" ) try : res [ \"month\" ] = dt . month except Exception as e : logger . error ( f \"Could not find month for { dt } (raw: { data } )\" ) try : res [ \"day\" ] = dt . day except Exception as e : logger . error ( f \"Could not find day for { dt } (raw: { data } )\" ) try : res [ \"weekday\" ] = dt . weekday () + 1 except Exception as e : logger . error ( f \"Could not find weekday for { dt } (raw: { data } )\" ) return res","title":"data.wrangle.datetime"},{"location":"references/data/wrangle/datetime/#data-wrangling-datetime","text":"","title":"Data - Wrangling - Datetime"},{"location":"references/data/wrangle/datetime/#haferml.data.wrangle.datetime.convert_to_datetime","text":"Convert input to datetime object. This is the last effort of converting input to datetime. The order of instance check is 1. datetime.datetime 2. str 3. float or int >>> handle_strange_dates(1531323212311) datetime(2018, 7, 11, 17, 33, 32, 311000) >>> handle_strange_dates(datetime(2085,1,1)) datetime(2050, 1, 1) Parameters: Name Type Description Default input_date input data of any possible format required input_tz input timezone, defaults to utc None output_tz output timezone, defaults to utc None Returns: Type Description datetime.datetime converted datetime format Source code in haferml/data/wrangle/datetime.py def convert_to_datetime ( input_date , dayfirst = None , input_tz = None , output_tz = None ): \"\"\" Convert input to *datetime* object. This is the last effort of converting input to datetime. The order of instance check is 1. datetime.datetime 2. str 3. float or int ``` >>> handle_strange_dates(1531323212311) datetime(2018, 7, 11, 17, 33, 32, 311000) >>> handle_strange_dates(datetime(2085,1,1)) datetime(2050, 1, 1) ``` :param input_date: input data of any possible format :param input_tz: input timezone, defaults to utc :param output_tz: output timezone, defaults to utc :return: converted datetime format :rtype: datetime.datetime \"\"\" if dayfirst is None : dayfirst = True if input_tz is None : input_tz = datetime . timezone . utc if output_tz is None : output_tz = datetime . timezone . utc res = None if isinstance ( input_date , datetime . datetime ): res = input_date if input_tz : res = res . replace ( tzinfo = input_tz ) if output_tz : res = res . astimezone ( output_tz ) elif isinstance ( input_date , str ): try : res = dateutil . parser . parse ( input_date , dayfirst = dayfirst ) if input_tz : res = res . replace ( tzinfo = input_tz ) if output_tz : res = res . astimezone ( output_tz ) except : logger . warning ( f \"Could not convert { input_date } to datetime!\" ) pass elif isinstance ( input_date , ( float , int )): try : res = datetime . datetime . utcfromtimestamp ( input_date / 1000 ) if input_tz : res = res . replace ( tzinfo = input_tz ) if output_tz : res = res . astimezone ( output_tz ) except : logger . warning ( f \"Could not convert { input_date } to datetime!\" ) pass else : raise Exception ( \"Could not convert {} to datetime: type {} is not handled\" . format ( input_date , type ( input_date ) ) ) return res","title":"convert_to_datetime()"},{"location":"references/data/wrangle/datetime/#haferml.data.wrangle.datetime.date_range_has_weekday","text":"date_range_has_weekday decides if the given date range contains weekday Parameters: Name Type Description Default dt_start datetime of the start of date range required dt_end datetime of the end of date range required Source code in haferml/data/wrangle/datetime.py def date_range_has_weekday ( dt_start , dt_end ): \"\"\" date_range_has_weekday decides if the given date range contains weekday :param dt_start: datetime of the start of date range :param dt_end: datetime of the end of date range \"\"\" res = [] if pd . isnull ( dt_start ) or pd . isnull ( dt_end ): logger . warning ( f \"date start end not specified: { dt_start } , { dt_end } \" ) return None if isinstance ( dt_start , str ): dt_start = pd . to_datetime ( dt_start ) if isinstance ( dt_end , str ): dt_end = pd . to_datetime ( dt_end ) for dt in pd . date_range ( dt_start , dt_end ): if dt . weekday () < 5 : res . append ( True ) else : res . append ( False ) return True in res","title":"date_range_has_weekday()"},{"location":"references/data/wrangle/datetime/#haferml.data.wrangle.datetime.unpack_datetime","text":"unpack_datetime converts datetime (string) to a dict of useful date information Source code in haferml/data/wrangle/datetime.py def unpack_datetime ( data ): \"\"\" unpack_datetime converts datetime (string) to a dict of useful date information \"\"\" res = {} dt = convert_to_datetime ( data , dayfirst = False ) if dt : try : res [ \"year\" ] = dt . year except Exception as e : logger . error ( f \"Could not find year for { dt } (raw: { data } )\" ) try : res [ \"month\" ] = dt . month except Exception as e : logger . error ( f \"Could not find month for { dt } (raw: { data } )\" ) try : res [ \"day\" ] = dt . day except Exception as e : logger . error ( f \"Could not find day for { dt } (raw: { data } )\" ) try : res [ \"weekday\" ] = dt . weekday () + 1 except Exception as e : logger . error ( f \"Could not find weekday for { dt } (raw: { data } )\" ) return res","title":"unpack_datetime()"},{"location":"references/data/wrangle/misc/","text":"Data - Wrangling - MISC \u00a4 convert_str_repr_to_list ( inp ) \u00a4 convert_str_repr_to_list concerts string representation of list to list Source code in haferml/data/wrangle/misc.py def convert_str_repr_to_list ( inp ): \"\"\" convert_str_repr_to_list concerts string representation of list to list \"\"\" res = [] if isinstance ( inp , str ): try : res = literal_eval ( inp ) except Exception as e : raise Exception ( f \"Could not convert { inp } to list\" ) elif isinstance ( inp , ( list , tuple , set )): res = list ( inp ) return res convert_str_repr_to_tuple ( inp ) \u00a4 convert_str_repr_to_tuple converts string representation of tuple to tuple Source code in haferml/data/wrangle/misc.py def convert_str_repr_to_tuple ( inp ): \"\"\" convert_str_repr_to_tuple converts string representation of tuple to tuple \"\"\" res = [] if isinstance ( inp , str ): try : res = literal_eval ( inp ) except Exception as e : raise Exception ( f \"Could not convert { inp } to list\" ) if isinstance ( inp , ( list , tuple , set )): res = tuple ( inp ) return res convert_to_bool ( data ) \u00a4 convert_to_bool converts input to bool type in python. The following values are converted to True: 'true' 'yes' '1' 'y' 1 The following values are converted to False: 'false' 'no' '0' 'n' 0 Parameters: Name Type Description Default data input data required Returns: Type Description bool boolean value of the input data Source code in haferml/data/wrangle/misc.py def convert_to_bool ( data ): \"\"\" convert_to_bool converts input to bool type in python. The following values are converted to True: 1. 'true' 2. 'yes' 3. '1' 4. 'y' 5. 1 The following values are converted to False: 1. 'false' 2. 'no' 3. '0' 4. 'n' 5. 0 :param data: input data :return: boolean value of the input data :rtype: bool \"\"\" res = None if data is None : return res elif isinstance ( data , bool ): res = data elif isinstance ( data , str ): if data . lower () . strip () in [ \"true\" , \"yes\" , \"1\" , \"y\" ]: res = True elif data . lower () . strip () in [ \"false\" , \"no\" , \"0\" , \"n\" ]: res = False else : res = None elif isinstance ( data , ( float , int )): res = bool ( data ) return res eu_float_string_to_float ( data ) \u00a4 eu_float_string_to_float converts strings in EU format to floats Parameters: Name Type Description Default data str string of the float in EU conventions required Returns: Type Description float converted float from the string Source code in haferml/data/wrangle/misc.py def eu_float_string_to_float ( data ): \"\"\" eu_float_string_to_float converts strings in EU format to floats :param data: string of the float in EU conventions :type data: str :return: converted float from the string :rtype: float \"\"\" if isinstance ( data , str ): res = data . replace ( \".\" , \"\" ) res = res . replace ( \",\" , \".\" ) try : res = float ( res ) except Exception as e : raise Exception ( f \"Could not convert string { data } to float: { e } \" ) else : raise TypeError ( \"Input data should be string\" ) return res get_all_paths_in_dict ( dic , path = None ) \u00a4 Retrieve all the possible paths in a nested dictionary. Warning List dictionaries under keys are not supported. test_dict_small = { \"etl\": { \"local\": \"this/is/local\", \"name\": \"my_data.parquet\", \"remote\": \"s3://my/remote\" }, \"model\": { \"artifacts\": { \"predict\": { \"local\": \"this/is/local/predict\", \"remote\": \"s3://my/remote/predict\" } } } } all_paths = get_all_paths_in_dict(test_dict_small, []) print(all_paths) We get [['etl', 'local'], ['etl', 'name'], ['etl', 'remote'], ['model', 'artifacts', 'predict', 'local'], ['model', 'artifacts', 'predict', 'remote']] Parameters: Name Type Description Default dic dict dictionary to be get data from required path list path of keys to extract value None Source code in haferml/data/wrangle/misc.py def get_all_paths_in_dict ( dic , path = None ): \"\"\" Retrieve all the possible paths in a nested dictionary. !!! warning List dictionaries under keys are not supported. ``` test_dict_small = { \"etl\": { \"local\": \"this/is/local\", \"name\": \"my_data.parquet\", \"remote\": \"s3://my/remote\" }, \"model\": { \"artifacts\": { \"predict\": { \"local\": \"this/is/local/predict\", \"remote\": \"s3://my/remote/predict\" } } } } all_paths = get_all_paths_in_dict(test_dict_small, []) print(all_paths) ``` We get ``` [['etl', 'local'], ['etl', 'name'], ['etl', 'remote'], ['model', 'artifacts', 'predict', 'local'], ['model', 'artifacts', 'predict', 'remote']] ``` :param dic: dictionary to be get data from :type dic: dict :param path: path of keys to extract value :type path: list \"\"\" if path is None : path = [] if not isinstance ( dic , dict ): return [ path ] else : ret = [] for k , v in dic . items (): ret . extend ( get_all_paths_in_dict ( v , path + [ k ])) return ret get_value_in_dict_recursively ( dictionary , path , ignore_path_fail = None ) \u00a4 Get value of a dictionary according to specified path (names) Parameters: Name Type Description Default dictionary dict input dictionary required path list path to the value to be obtained This function always returns the value or None. >>> get_value_in_dict_recursively({'lvl_1':{'lvl_2':{'lvl_3':'lvl_3_value'}}},['lvl_1','lvl_3']) {'lvl_3':'lvl_3_value'} >>> get_value_in_dict_recursively({1:{2:{3:'hi'}}},[1,'2',3]) {'hi'} required Source code in haferml/data/wrangle/misc.py def get_value_in_dict_recursively ( dictionary , path , ignore_path_fail = None ): \"\"\" Get value of a dictionary according to specified path (names) :param dict dictionary: input dictionary :param list path: path to the value to be obtained This function always returns the value or None. ``` >>> get_value_in_dict_recursively({'lvl_1':{'lvl_2':{'lvl_3':'lvl_3_value'}}},['lvl_1','lvl_3']) {'lvl_3':'lvl_3_value'} >>> get_value_in_dict_recursively({1:{2:{3:'hi'}}},[1,'2',3]) {'hi'} ``` \"\"\" if ignore_path_fail is None : ignore_path_fail = True if isinstance ( path , list ): path_temp = path . copy () elif isinstance ( path , tuple ): path_temp = list ( path ) . copy () else : logger . warning ( f \"path is not list or tuple, converting to list: { path } \" ) path_temp = [ path ] . copy () if len ( path_temp ) > 1 : pop = path_temp . pop ( 0 ) try : pop = int ( pop ) except ValueError : if ignore_path_fail : logger . warning ( f \"can not get path\" ) pass else : raise Exception ( f \"specified path ( { path } ) is not acceptable\" ) try : return get_value_in_dict_recursively ( dictionary [ pop ], path_temp ) except : logger . debug ( f \"did not get values for { pop } \" ) return None elif len ( path_temp ) == 0 : return None else : try : val = int ( path_temp [ 0 ]) except : val = path_temp [ 0 ] try : return dictionary [ val ] except KeyError : logger . error ( f \"KeyError: Could not find { path_temp [ 0 ] } \" ) return None except TypeError : logger . error ( f \"TypeError: Could not find { path_temp [ 0 ] } \" ) return None update_dict_recursively ( dictionary , key_path , value ) \u00a4 update or insert values to a dictionary recursively. >>> update_dict_recursively({}, ['a', 'b', 1, 2], 'this_value') {'a': {'b': {1: {2: 'this_value'}}}} Parameters: Name Type Description Default dictionary dict the dictionary to be inserted into required key_path list the path for the insertion value required value value to be inserted required Returns: Type Description a dictionary with the inserted value Source code in haferml/data/wrangle/misc.py def update_dict_recursively ( dictionary , key_path , value ): \"\"\" update or insert values to a dictionary recursively. ``` >>> update_dict_recursively({}, ['a', 'b', 1, 2], 'this_value') {'a': {'b': {1: {2: 'this_value'}}}} ``` :param dict dictionary: the dictionary to be inserted into :param list key_path: the path for the insertion value :param value: value to be inserted :returns: a dictionary with the inserted value \"\"\" sub_dictionary = dictionary for key in key_path [: - 1 ]: if key not in sub_dictionary : sub_dictionary [ key ] = {} sub_dictionary = sub_dictionary [ key ] sub_dictionary [ key_path [ - 1 ]] = value return dictionary","title":"data.wrangle.misc"},{"location":"references/data/wrangle/misc/#data-wrangling-misc","text":"","title":"Data - Wrangling - MISC"},{"location":"references/data/wrangle/misc/#haferml.data.wrangle.misc.convert_str_repr_to_list","text":"convert_str_repr_to_list concerts string representation of list to list Source code in haferml/data/wrangle/misc.py def convert_str_repr_to_list ( inp ): \"\"\" convert_str_repr_to_list concerts string representation of list to list \"\"\" res = [] if isinstance ( inp , str ): try : res = literal_eval ( inp ) except Exception as e : raise Exception ( f \"Could not convert { inp } to list\" ) elif isinstance ( inp , ( list , tuple , set )): res = list ( inp ) return res","title":"convert_str_repr_to_list()"},{"location":"references/data/wrangle/misc/#haferml.data.wrangle.misc.convert_str_repr_to_tuple","text":"convert_str_repr_to_tuple converts string representation of tuple to tuple Source code in haferml/data/wrangle/misc.py def convert_str_repr_to_tuple ( inp ): \"\"\" convert_str_repr_to_tuple converts string representation of tuple to tuple \"\"\" res = [] if isinstance ( inp , str ): try : res = literal_eval ( inp ) except Exception as e : raise Exception ( f \"Could not convert { inp } to list\" ) if isinstance ( inp , ( list , tuple , set )): res = tuple ( inp ) return res","title":"convert_str_repr_to_tuple()"},{"location":"references/data/wrangle/misc/#haferml.data.wrangle.misc.convert_to_bool","text":"convert_to_bool converts input to bool type in python. The following values are converted to True: 'true' 'yes' '1' 'y' 1 The following values are converted to False: 'false' 'no' '0' 'n' 0 Parameters: Name Type Description Default data input data required Returns: Type Description bool boolean value of the input data Source code in haferml/data/wrangle/misc.py def convert_to_bool ( data ): \"\"\" convert_to_bool converts input to bool type in python. The following values are converted to True: 1. 'true' 2. 'yes' 3. '1' 4. 'y' 5. 1 The following values are converted to False: 1. 'false' 2. 'no' 3. '0' 4. 'n' 5. 0 :param data: input data :return: boolean value of the input data :rtype: bool \"\"\" res = None if data is None : return res elif isinstance ( data , bool ): res = data elif isinstance ( data , str ): if data . lower () . strip () in [ \"true\" , \"yes\" , \"1\" , \"y\" ]: res = True elif data . lower () . strip () in [ \"false\" , \"no\" , \"0\" , \"n\" ]: res = False else : res = None elif isinstance ( data , ( float , int )): res = bool ( data ) return res","title":"convert_to_bool()"},{"location":"references/data/wrangle/misc/#haferml.data.wrangle.misc.eu_float_string_to_float","text":"eu_float_string_to_float converts strings in EU format to floats Parameters: Name Type Description Default data str string of the float in EU conventions required Returns: Type Description float converted float from the string Source code in haferml/data/wrangle/misc.py def eu_float_string_to_float ( data ): \"\"\" eu_float_string_to_float converts strings in EU format to floats :param data: string of the float in EU conventions :type data: str :return: converted float from the string :rtype: float \"\"\" if isinstance ( data , str ): res = data . replace ( \".\" , \"\" ) res = res . replace ( \",\" , \".\" ) try : res = float ( res ) except Exception as e : raise Exception ( f \"Could not convert string { data } to float: { e } \" ) else : raise TypeError ( \"Input data should be string\" ) return res","title":"eu_float_string_to_float()"},{"location":"references/data/wrangle/misc/#haferml.data.wrangle.misc.get_all_paths_in_dict","text":"Retrieve all the possible paths in a nested dictionary. Warning List dictionaries under keys are not supported. test_dict_small = { \"etl\": { \"local\": \"this/is/local\", \"name\": \"my_data.parquet\", \"remote\": \"s3://my/remote\" }, \"model\": { \"artifacts\": { \"predict\": { \"local\": \"this/is/local/predict\", \"remote\": \"s3://my/remote/predict\" } } } } all_paths = get_all_paths_in_dict(test_dict_small, []) print(all_paths) We get [['etl', 'local'], ['etl', 'name'], ['etl', 'remote'], ['model', 'artifacts', 'predict', 'local'], ['model', 'artifacts', 'predict', 'remote']] Parameters: Name Type Description Default dic dict dictionary to be get data from required path list path of keys to extract value None Source code in haferml/data/wrangle/misc.py def get_all_paths_in_dict ( dic , path = None ): \"\"\" Retrieve all the possible paths in a nested dictionary. !!! warning List dictionaries under keys are not supported. ``` test_dict_small = { \"etl\": { \"local\": \"this/is/local\", \"name\": \"my_data.parquet\", \"remote\": \"s3://my/remote\" }, \"model\": { \"artifacts\": { \"predict\": { \"local\": \"this/is/local/predict\", \"remote\": \"s3://my/remote/predict\" } } } } all_paths = get_all_paths_in_dict(test_dict_small, []) print(all_paths) ``` We get ``` [['etl', 'local'], ['etl', 'name'], ['etl', 'remote'], ['model', 'artifacts', 'predict', 'local'], ['model', 'artifacts', 'predict', 'remote']] ``` :param dic: dictionary to be get data from :type dic: dict :param path: path of keys to extract value :type path: list \"\"\" if path is None : path = [] if not isinstance ( dic , dict ): return [ path ] else : ret = [] for k , v in dic . items (): ret . extend ( get_all_paths_in_dict ( v , path + [ k ])) return ret","title":"get_all_paths_in_dict()"},{"location":"references/data/wrangle/misc/#haferml.data.wrangle.misc.get_value_in_dict_recursively","text":"Get value of a dictionary according to specified path (names) Parameters: Name Type Description Default dictionary dict input dictionary required path list path to the value to be obtained This function always returns the value or None. >>> get_value_in_dict_recursively({'lvl_1':{'lvl_2':{'lvl_3':'lvl_3_value'}}},['lvl_1','lvl_3']) {'lvl_3':'lvl_3_value'} >>> get_value_in_dict_recursively({1:{2:{3:'hi'}}},[1,'2',3]) {'hi'} required Source code in haferml/data/wrangle/misc.py def get_value_in_dict_recursively ( dictionary , path , ignore_path_fail = None ): \"\"\" Get value of a dictionary according to specified path (names) :param dict dictionary: input dictionary :param list path: path to the value to be obtained This function always returns the value or None. ``` >>> get_value_in_dict_recursively({'lvl_1':{'lvl_2':{'lvl_3':'lvl_3_value'}}},['lvl_1','lvl_3']) {'lvl_3':'lvl_3_value'} >>> get_value_in_dict_recursively({1:{2:{3:'hi'}}},[1,'2',3]) {'hi'} ``` \"\"\" if ignore_path_fail is None : ignore_path_fail = True if isinstance ( path , list ): path_temp = path . copy () elif isinstance ( path , tuple ): path_temp = list ( path ) . copy () else : logger . warning ( f \"path is not list or tuple, converting to list: { path } \" ) path_temp = [ path ] . copy () if len ( path_temp ) > 1 : pop = path_temp . pop ( 0 ) try : pop = int ( pop ) except ValueError : if ignore_path_fail : logger . warning ( f \"can not get path\" ) pass else : raise Exception ( f \"specified path ( { path } ) is not acceptable\" ) try : return get_value_in_dict_recursively ( dictionary [ pop ], path_temp ) except : logger . debug ( f \"did not get values for { pop } \" ) return None elif len ( path_temp ) == 0 : return None else : try : val = int ( path_temp [ 0 ]) except : val = path_temp [ 0 ] try : return dictionary [ val ] except KeyError : logger . error ( f \"KeyError: Could not find { path_temp [ 0 ] } \" ) return None except TypeError : logger . error ( f \"TypeError: Could not find { path_temp [ 0 ] } \" ) return None","title":"get_value_in_dict_recursively()"},{"location":"references/data/wrangle/misc/#haferml.data.wrangle.misc.update_dict_recursively","text":"update or insert values to a dictionary recursively. >>> update_dict_recursively({}, ['a', 'b', 1, 2], 'this_value') {'a': {'b': {1: {2: 'this_value'}}}} Parameters: Name Type Description Default dictionary dict the dictionary to be inserted into required key_path list the path for the insertion value required value value to be inserted required Returns: Type Description a dictionary with the inserted value Source code in haferml/data/wrangle/misc.py def update_dict_recursively ( dictionary , key_path , value ): \"\"\" update or insert values to a dictionary recursively. ``` >>> update_dict_recursively({}, ['a', 'b', 1, 2], 'this_value') {'a': {'b': {1: {2: 'this_value'}}}} ``` :param dict dictionary: the dictionary to be inserted into :param list key_path: the path for the insertion value :param value: value to be inserted :returns: a dictionary with the inserted value \"\"\" sub_dictionary = dictionary for key in key_path [: - 1 ]: if key not in sub_dictionary : sub_dictionary [ key ] = {} sub_dictionary = sub_dictionary [ key ] sub_dictionary [ key_path [ - 1 ]] = value return dictionary","title":"update_dict_recursively()"},{"location":"references/etl/","text":"ETL \u00a4","title":"Index"},{"location":"references/etl/#etl","text":"","title":"ETL"},{"location":"references/etl/extract/","text":"ETL - Extract \u00a4 Utilities for extraction of data. Warning WIP! Many of the functions are still being tested in the Lab module.","title":"etl.extract"},{"location":"references/etl/extract/#etl-extract","text":"Utilities for extraction of data. Warning WIP! Many of the functions are still being tested in the Lab module.","title":"ETL - Extract"},{"location":"references/etl/transform/","text":"ETL - Transform \u00a4","title":"Index"},{"location":"references/etl/transform/#etl-transform","text":"","title":"ETL - Transform"},{"location":"references/etl/transform/ingredients/","text":"ETL - Transform - Ingredients \u00a4","title":"etl.transform.ingredients"},{"location":"references/etl/transform/ingredients/#etl-transform-ingredients","text":"","title":"ETL - Transform - Ingredients"},{"location":"references/etl/transform/pipeline/","text":"ETL - Transform - Pipeline \u00a4 Transformer \u00a4 Base Transformer of a single data record. This transformer can use a schema specification schema and also built-in methods to transform the data. A schema can have the following structure. schema = [ { \"column_name\": \"abc\", \"type\": \"string\" }, { \"column_name\": \"def\", \"type\": \"list\" } ] The \"type\" values will be used by the universal transformer method _universal_transformer . A method with the name like _transformer__abc will be used to transform the column abc . If such method exists, this method will be used otherwise the universal transformer will be used and the transformation will be done based on the \"type\" of the column specified in the schema. _get_transformers ( self ) private \u00a4 _get_transformers extracts the list of transformers Source code in haferml/etl/transform/pipeline.py def _get_transformers ( self ): \"\"\" _get_transformers extracts the list of transformers \"\"\" re_transformer_name = re . compile ( \"^_transformer__(.*?)$\" ) transformers = {} all_methods = dict ( inspect . getmembers ( self )) for i in all_methods : if i . startswith ( \"_transformer__\" ): transformer_name = re_transformer_name . findall ( i )[ 0 ] transformer_method_i = all_methods . get ( i ) transformers [ transformer_name ] = transformer_method_i logger . debug ( \"All methods: {} \" . format ( all_methods )) logger . info ( \"All predefined transformers: {} \" . format ( transformers )) for i in self . transformer_schema : logger . info ( \"this transformer schema: {} \" . format ( i )) i_val = self . transformer_schema . get ( i , {}) if i in transformers : i_val [ \"transformer\" ] = transformers . get ( i ) logger . info ( \"Has predefined transformer for {} \" . format ( i )) else : logger . info ( \"Using default transformer for {} ; format: {} \" . format ( i , i_val . get ( \"type\" ) ) ) to_format_type = i_val . get ( \"type\" ) i_val [ \"transformer\" ] = self . _universal_transformer ( to_format = to_format_type ) self . transformer_schema [ i ] = i_val _schema_to_utils ( self ) private \u00a4 _schema_to_utils converts some utility schemas using the input full schema Source code in haferml/etl/transform/pipeline.py def _schema_to_utils ( self ): \"\"\" _schema_to_utils converts some utility schemas using the input full schema \"\"\" self . column_rename_schema = { i . get ( \"column_name\" ): i . get ( \"column_name\" ) for i in self . schema } # build transformer schema self . transformer_schema = { i . get ( \"column_name\" ): { \"type\" : i . get ( \"type\" )} for i in self . schema } self . _get_transformers () # enhances the schema with the transformer function _universal_transformer ( to_format , from_format = None ) private staticmethod \u00a4 _general_transformer is to be used if a specific transformer of the column is not found Source code in haferml/etl/transform/pipeline.py @staticmethod def _universal_transformer ( to_format , from_format = None ): \"\"\" _general_transformer is to be used if a specific transformer of the column is not found \"\"\" def transformer ( data ): \"\"\" transformer is the actual transformer \"\"\" if pd . isnull ( data ): return None logger . debug ( f \"Transforming { data } to format { to_format } \" ) if to_format . lower () in ( \"str\" , \"string\" ): try : res = str ( data ) res = res . strip () except Exception as e : raise Exception ( \"Could not convert {} to str\" . format ( data )) elif to_format . lower () == \"int\" : if isinstance ( data , str ): if ( \".\" in data ) or ( \",\" in data ): data = wlg . misc . eu_float_string_to_float ( data ) try : res = int ( float ( data )) except Exception as e : raise Exception ( \"Could not convert {} to float->int\" . format ( data )) elif to_format . lower () == \"float\" : if isinstance ( data , str ): if ( \".\" in data ) or ( \",\" in data ): data = wlg . misc . eu_float_string_to_float ( data ) try : res = float ( data ) except Exception as e : raise Exception ( \"Could not convert {} to float\" . format ( data )) elif to_format . lower () == \"datetime\" : res = wlg . datetime . convert_to_datetime ( data , dayfirst = False ) elif to_format . lower () == \"date\" : res = wlg . datetime . convert_to_date ( data ) elif to_format . lower () == \"bool\" : res = wlg . misc . convert_to_bool ( data ) elif to_format . lower () == \"list\" : res = wlg . misc . convert_str_repr_to_list ( data ) else : raise Exception ( f \"Can not transform { data } ; No transformer defined for the format: { to_format } \" ) return res return transformer transform ( self , record ) \u00a4 transform transforms the json (list of dict data) into standardized format Source code in haferml/etl/transform/pipeline.py def transform ( self , record ): \"\"\" transform transforms the json (list of dict data) into standardized format \"\"\" # sometime the transformations requires other fields # we need to set self.record to access all the fields self . record = record . copy () try : for key , val in record . items (): try : val = self . transformer_schema [ key ][ \"transformer\" ]( val ) record [ key ] = val except Exception as e : logger . error ( \"Failed to transform key, val: {} , {} ; schema is {} ; e {} \" . format ( key , val , self . transformer_schema [ key ], e ) ) except Exception as e : raise Exception ( \"Failed to transform record: {} ;error is {} \" . format ( record , e ) ) return record","title":"etl.transform.pipeline"},{"location":"references/etl/transform/pipeline/#etl-transform-pipeline","text":"","title":"ETL - Transform - Pipeline"},{"location":"references/etl/transform/pipeline/#haferml.etl.transform.pipeline.Transformer","text":"Base Transformer of a single data record. This transformer can use a schema specification schema and also built-in methods to transform the data. A schema can have the following structure. schema = [ { \"column_name\": \"abc\", \"type\": \"string\" }, { \"column_name\": \"def\", \"type\": \"list\" } ] The \"type\" values will be used by the universal transformer method _universal_transformer . A method with the name like _transformer__abc will be used to transform the column abc . If such method exists, this method will be used otherwise the universal transformer will be used and the transformation will be done based on the \"type\" of the column specified in the schema.","title":"Transformer"},{"location":"references/etl/transform/pipeline/#haferml.etl.transform.pipeline.Transformer._get_transformers","text":"_get_transformers extracts the list of transformers Source code in haferml/etl/transform/pipeline.py def _get_transformers ( self ): \"\"\" _get_transformers extracts the list of transformers \"\"\" re_transformer_name = re . compile ( \"^_transformer__(.*?)$\" ) transformers = {} all_methods = dict ( inspect . getmembers ( self )) for i in all_methods : if i . startswith ( \"_transformer__\" ): transformer_name = re_transformer_name . findall ( i )[ 0 ] transformer_method_i = all_methods . get ( i ) transformers [ transformer_name ] = transformer_method_i logger . debug ( \"All methods: {} \" . format ( all_methods )) logger . info ( \"All predefined transformers: {} \" . format ( transformers )) for i in self . transformer_schema : logger . info ( \"this transformer schema: {} \" . format ( i )) i_val = self . transformer_schema . get ( i , {}) if i in transformers : i_val [ \"transformer\" ] = transformers . get ( i ) logger . info ( \"Has predefined transformer for {} \" . format ( i )) else : logger . info ( \"Using default transformer for {} ; format: {} \" . format ( i , i_val . get ( \"type\" ) ) ) to_format_type = i_val . get ( \"type\" ) i_val [ \"transformer\" ] = self . _universal_transformer ( to_format = to_format_type ) self . transformer_schema [ i ] = i_val","title":"_get_transformers()"},{"location":"references/etl/transform/pipeline/#haferml.etl.transform.pipeline.Transformer._schema_to_utils","text":"_schema_to_utils converts some utility schemas using the input full schema Source code in haferml/etl/transform/pipeline.py def _schema_to_utils ( self ): \"\"\" _schema_to_utils converts some utility schemas using the input full schema \"\"\" self . column_rename_schema = { i . get ( \"column_name\" ): i . get ( \"column_name\" ) for i in self . schema } # build transformer schema self . transformer_schema = { i . get ( \"column_name\" ): { \"type\" : i . get ( \"type\" )} for i in self . schema } self . _get_transformers () # enhances the schema with the transformer function","title":"_schema_to_utils()"},{"location":"references/etl/transform/pipeline/#haferml.etl.transform.pipeline.Transformer._universal_transformer","text":"_general_transformer is to be used if a specific transformer of the column is not found Source code in haferml/etl/transform/pipeline.py @staticmethod def _universal_transformer ( to_format , from_format = None ): \"\"\" _general_transformer is to be used if a specific transformer of the column is not found \"\"\" def transformer ( data ): \"\"\" transformer is the actual transformer \"\"\" if pd . isnull ( data ): return None logger . debug ( f \"Transforming { data } to format { to_format } \" ) if to_format . lower () in ( \"str\" , \"string\" ): try : res = str ( data ) res = res . strip () except Exception as e : raise Exception ( \"Could not convert {} to str\" . format ( data )) elif to_format . lower () == \"int\" : if isinstance ( data , str ): if ( \".\" in data ) or ( \",\" in data ): data = wlg . misc . eu_float_string_to_float ( data ) try : res = int ( float ( data )) except Exception as e : raise Exception ( \"Could not convert {} to float->int\" . format ( data )) elif to_format . lower () == \"float\" : if isinstance ( data , str ): if ( \".\" in data ) or ( \",\" in data ): data = wlg . misc . eu_float_string_to_float ( data ) try : res = float ( data ) except Exception as e : raise Exception ( \"Could not convert {} to float\" . format ( data )) elif to_format . lower () == \"datetime\" : res = wlg . datetime . convert_to_datetime ( data , dayfirst = False ) elif to_format . lower () == \"date\" : res = wlg . datetime . convert_to_date ( data ) elif to_format . lower () == \"bool\" : res = wlg . misc . convert_to_bool ( data ) elif to_format . lower () == \"list\" : res = wlg . misc . convert_str_repr_to_list ( data ) else : raise Exception ( f \"Can not transform { data } ; No transformer defined for the format: { to_format } \" ) return res return transformer","title":"_universal_transformer()"},{"location":"references/etl/transform/pipeline/#haferml.etl.transform.pipeline.Transformer.transform","text":"transform transforms the json (list of dict data) into standardized format Source code in haferml/etl/transform/pipeline.py def transform ( self , record ): \"\"\" transform transforms the json (list of dict data) into standardized format \"\"\" # sometime the transformations requires other fields # we need to set self.record to access all the fields self . record = record . copy () try : for key , val in record . items (): try : val = self . transformer_schema [ key ][ \"transformer\" ]( val ) record [ key ] = val except Exception as e : logger . error ( \"Failed to transform key, val: {} , {} ; schema is {} ; e {} \" . format ( key , val , self . transformer_schema [ key ], e ) ) except Exception as e : raise Exception ( \"Failed to transform record: {} ;error is {} \" . format ( record , e ) ) return record","title":"transform()"},{"location":"references/lab/","text":"Lab \u00a4 Experimental codes are being tested here.","title":"Lab"},{"location":"references/lab/#lab","text":"Experimental codes are being tested here.","title":"Lab"},{"location":"references/model/","text":"Model \u00a4","title":"Index"},{"location":"references/model/#model","text":"","title":"Model"},{"location":"references/model/dataset/","text":"Model - DataSet \u00a4 DataSet \u00a4 DataSet to be used in a model workflow. Warning This class is a proposed framework. There are many member functions to be implemented if you are using the DataSet class. In most projects, we do not need to use this class. We have a class DataSetX which is a more detailed implementation in our model.pipeline module. The interface of this class in a workflow is the create_train_test_datasets method. Depending on the requirements, one can also implement a method _export_train_test_data to save the dataset locally or remotely. _export_train_test_data ( self ) private \u00a4 _export_train_test_data saves train and test datasets. Warning Please implement this method. Source code in haferml/model/dataset.py def _export_train_test_data ( self ): \"\"\" _export_train_test_data saves train and test datasets. !!! warning Please implement this method. \"\"\" ... _save_data ( dataframe , destination ) private staticmethod \u00a4 _save_data is dummy method method to save the data to a local destination. Warning Please implement this method. Parameters: Name Type Description Default dataframe pandas.DataFrame a pandas dataframe that holds the data required destination str destination of the data on a local machine required Source code in haferml/model/dataset.py @staticmethod def _save_data ( dataframe , destination ): \"\"\" `_save_data` is dummy method method to save the data to a local destination. !!! warning Please implement this method. :param dataframe: a pandas dataframe that holds the data :type dataframe: pandas.DataFrame :param destination: destination of the data on a local machine :type destination: str \"\"\" logger . debug ( f \"Saving data to { destination } ...\" ) dataframe . to_parquet ( destination ) create_train_test_datasets ( self ) \u00a4 create_train_test_datasets is our interface to our workflow. Warning Please implement this method. One could call the _export_train_test_data method to export the dataset locally or remotely. Source code in haferml/model/dataset.py def create_train_test_datasets ( self ): \"\"\" create_train_test_datasets is our interface to our workflow. !!! warning Please implement this method. One could call the `_export_train_test_data` method to export the dataset locally or remotely. \"\"\" ...","title":"model.dataset"},{"location":"references/model/dataset/#model-dataset","text":"","title":"Model - DataSet"},{"location":"references/model/dataset/#haferml.model.dataset.DataSet","text":"DataSet to be used in a model workflow. Warning This class is a proposed framework. There are many member functions to be implemented if you are using the DataSet class. In most projects, we do not need to use this class. We have a class DataSetX which is a more detailed implementation in our model.pipeline module. The interface of this class in a workflow is the create_train_test_datasets method. Depending on the requirements, one can also implement a method _export_train_test_data to save the dataset locally or remotely.","title":"DataSet"},{"location":"references/model/dataset/#haferml.model.dataset.DataSet._export_train_test_data","text":"_export_train_test_data saves train and test datasets. Warning Please implement this method. Source code in haferml/model/dataset.py def _export_train_test_data ( self ): \"\"\" _export_train_test_data saves train and test datasets. !!! warning Please implement this method. \"\"\" ...","title":"_export_train_test_data()"},{"location":"references/model/dataset/#haferml.model.dataset.DataSet._save_data","text":"_save_data is dummy method method to save the data to a local destination. Warning Please implement this method. Parameters: Name Type Description Default dataframe pandas.DataFrame a pandas dataframe that holds the data required destination str destination of the data on a local machine required Source code in haferml/model/dataset.py @staticmethod def _save_data ( dataframe , destination ): \"\"\" `_save_data` is dummy method method to save the data to a local destination. !!! warning Please implement this method. :param dataframe: a pandas dataframe that holds the data :type dataframe: pandas.DataFrame :param destination: destination of the data on a local machine :type destination: str \"\"\" logger . debug ( f \"Saving data to { destination } ...\" ) dataframe . to_parquet ( destination )","title":"_save_data()"},{"location":"references/model/dataset/#haferml.model.dataset.DataSet.create_train_test_datasets","text":"create_train_test_datasets is our interface to our workflow. Warning Please implement this method. One could call the _export_train_test_data method to export the dataset locally or remotely. Source code in haferml/model/dataset.py def create_train_test_datasets ( self ): \"\"\" create_train_test_datasets is our interface to our workflow. !!! warning Please implement this method. One could call the `_export_train_test_data` method to export the dataset locally or remotely. \"\"\" ...","title":"create_train_test_datasets()"},{"location":"references/model/modelset/","text":"Model - ModelSet \u00a4 ModelSet \u00a4 ModelSet is the core of the model including hyperparameters. Warning This class is a proposed framework. There are many member functions to be implemented if you are using the ModelSet class. In most projects, we do not need to use this class. We have a class ModelSetX which is a more detailed implementation in our model.pipeline module. hyperparameters property readonly \u00a4 hyperparameters specifies the hyperparameters. This is a property. create_model ( self ) \u00a4 create_model creates the model and updates the property self.model . Source code in haferml/model/modelset.py def create_model ( self ): \"\"\" `create_model` creates the model and updates the property `self.model`. \"\"\" ...","title":"model.modelset"},{"location":"references/model/modelset/#model-modelset","text":"","title":"Model - ModelSet"},{"location":"references/model/modelset/#haferml.model.modelset.ModelSet","text":"ModelSet is the core of the model including hyperparameters. Warning This class is a proposed framework. There are many member functions to be implemented if you are using the ModelSet class. In most projects, we do not need to use this class. We have a class ModelSetX which is a more detailed implementation in our model.pipeline module.","title":"ModelSet"},{"location":"references/model/modelset/#haferml.model.modelset.ModelSet.hyperparameters","text":"hyperparameters specifies the hyperparameters. This is a property.","title":"hyperparameters"},{"location":"references/model/modelset/#haferml.model.modelset.ModelSet.create_model","text":"create_model creates the model and updates the property self.model . Source code in haferml/model/modelset.py def create_model ( self ): \"\"\" `create_model` creates the model and updates the property `self.model`. \"\"\" ...","title":"create_model()"},{"location":"references/model/pipeline/","text":"Model - Workflow \u00a4 DataSetX ( DataSet ) \u00a4 DataSetX class deals with the dataset to be used in a model Parameters: Name Type Description Default config dict a dictionary that contains the configurations. required base_folder str working directory where all the artifacts are being perserved. required _export_train_test_data ( self ) private \u00a4 _export_train_test_data saves train and test datasets Source code in haferml/model/pipeline.py def _export_train_test_data ( self ): \"\"\" _export_train_test_data saves train and test datasets \"\"\" dataset_folder = self . artifacts [ \"dataset\" ][ \"local_absolute\" ] ## Save the train and test datasets logger . info ( \"Export test and train data\" ) self . _save_data ( self . X_train , os . path . join ( dataset_folder , \"model_X_train.parquet\" ) ) self . _save_data ( self . X_test , os . path . join ( dataset_folder , \"model_X_test.parquet\" ) ) self . _save_data ( pd . DataFrame ( self . y_train , columns = self . targets ), os . path . join ( dataset_folder , \"model_y_train.parquet\" ), ) self . _save_data ( pd . DataFrame ( self . y_test , columns = self . targets ), os . path . join ( dataset_folder , \"model_y_test.parquet\" ), ) # Save dataset locally self . _save_data ( self . data , os . path . join ( dataset_folder , \"dataset.parquet\" )) _save_data ( dataframe , destination ) private staticmethod \u00a4 _save_data saves the dataframe locally. Parameters: Name Type Description Default dataframe pandas.DataFrame dataframe to be saved required destination str where the data is saved required Source code in haferml/model/pipeline.py @staticmethod def _save_data ( dataframe , destination ): \"\"\" `_save_data` saves the dataframe locally. :param dataframe: dataframe to be saved :type dataframe: pandas.DataFrame :param destination: where the data is saved :type destination: str \"\"\" logger . info ( \"Export test and train data\" ) dataframe . to_parquet ( destination ) create_train_test_datasets ( self , dataframe ) \u00a4 create_train_test_datasets will create self.data: the full input data right before train test split self.X_train, self.y_train, self.X_test, self.y_test Parameters: Name Type Description Default dataframe pandas.DataFrame the dataframe to be splitted. required Source code in haferml/model/pipeline.py def create_train_test_datasets ( self , dataframe ): \"\"\" create_train_test_datasets will create - self.data: the full input data right before train test split - self.X_train, self.y_train, self.X_test, self.y_test :param dataframe: the dataframe to be splitted. :type dataframe: pandas.DataFrame \"\"\" raise Exception ( \"create_train_test_dataset has not yet been implemented!\" ) ModelSetX ( ModelSet ) \u00a4 The core of the model including hyperparameters. Parameters: Name Type Description Default config dict a dictionary that contains the configurations. required base_folder str working directory where all the artifacts are being perserved. required hyperparameters property readonly \u00a4 hyperparameters specifies the hyperparameters. This is a property. _set_hyperparameters ( self ) private \u00a4 _set_hyperparameters creates hyperpamater grid Source code in haferml/model/pipeline.py def _set_hyperparameters ( self ): \"\"\" _set_hyperparameters creates hyperpamater grid \"\"\" ... create_model ( self ) \u00a4 create_model creates the model and updates the property self.model . Source code in haferml/model/pipeline.py def create_model ( self ): ... ModelWorkflowX ( ModelWorkflow ) \u00a4 ModelWorkflowX class that holds DataSetX and ModelSetX Parameters: Name Type Description Default config dict a dictionary that contains the configs. required dataset haferml.model.DataSet a DataSet object that contains the data and provides a create_train_test_datasets method. required modelset haferml.model.ModelSet a ModelSet object that contains the model as well as the hyperparameters and a create_model . required base_folder str working directory where all the artifacts are being perserved. required export_results ( self ) \u00a4 export_results saves the necessary artifacts Source code in haferml/model/pipeline.py def export_results ( self ): \"\"\" export_results saves the necessary artifacts \"\"\" model_artifacts = self . artifacts [ \"model\" ] model_folder = model_artifacts [ \"local_absolute\" ] model_path = model_artifacts [ \"name_absolute\" ] if not os . path . exists ( model_folder ): os . makedirs ( model_folder ) logger . info ( \"Preserving models ...\" ) joblib . dump ( self . ModelSet . model , model_path ) logger . info ( \"Perserving logs ...\" ) log_file_path = f \" { model_path } .log\" logger . info ( f \"Save log file to { log_file_path } \" ) with open ( log_file_path , \"a+\" ) as fp : json . dump ( self . report , fp , default = isoencode ) fp . write ( \" \\n \" ) logger . info ( f \"Saved logs\" ) fit_and_report ( self ) \u00a4 _fit_and_report fits the model using input data and generate reports Source code in haferml/model/pipeline.py def fit_and_report ( self ): \"\"\" _fit_and_report fits the model using input data and generate reports \"\"\" logger . info ( \"Fitting the model ...\" ) logger . debug ( \"Shape of train data: \\n \" f \"X_train: { self . DataSet . X_train . shape } , { self . DataSet . X_train . sample ( 3 ) } \\n \" f \"y_train: { self . DataSet . y_train . shape } , { self . DataSet . y_train . sample ( 3 ) } \" ) self . ModelSet . model . fit ( self . DataSet . X_train . squeeze (), self . DataSet . y_train . squeeze () ) self . report = { \"hyperparameters\" : self . ModelSet . hyperparameters , \"best_params\" : self . ModelSet . model . best_params_ , \"cv_results\" : self . ModelSet . model . cv_results_ , } logger . debug ( self . report ) train ( self , dataset ) \u00a4 train connects the training workflow Parameters: Name Type Description Default dataset pandas.DataFrame dataframe being used to train the model required Source code in haferml/model/pipeline.py def train ( self , dataset ): \"\"\" train connects the training workflow :param dataset: dataframe being used to train the model :type dataset: pandas.DataFrame \"\"\" logger . info ( \"1. Create train test datasets\" ) self . DataSet . create_train_test_datasets ( dataset ) logger . info ( \"2. Create model\" ) self . ModelSet . create_model () logger . info ( \"3. Fit model and report\" ) self . fit_and_report () logger . info ( \"4. Export results\" ) self . export_results ()","title":"model.pipeline"},{"location":"references/model/pipeline/#model-workflow","text":"","title":"Model - Workflow"},{"location":"references/model/pipeline/#haferml.model.pipeline.DataSetX","text":"DataSetX class deals with the dataset to be used in a model Parameters: Name Type Description Default config dict a dictionary that contains the configurations. required base_folder str working directory where all the artifacts are being perserved. required","title":"DataSetX"},{"location":"references/model/pipeline/#haferml.model.pipeline.DataSetX._export_train_test_data","text":"_export_train_test_data saves train and test datasets Source code in haferml/model/pipeline.py def _export_train_test_data ( self ): \"\"\" _export_train_test_data saves train and test datasets \"\"\" dataset_folder = self . artifacts [ \"dataset\" ][ \"local_absolute\" ] ## Save the train and test datasets logger . info ( \"Export test and train data\" ) self . _save_data ( self . X_train , os . path . join ( dataset_folder , \"model_X_train.parquet\" ) ) self . _save_data ( self . X_test , os . path . join ( dataset_folder , \"model_X_test.parquet\" ) ) self . _save_data ( pd . DataFrame ( self . y_train , columns = self . targets ), os . path . join ( dataset_folder , \"model_y_train.parquet\" ), ) self . _save_data ( pd . DataFrame ( self . y_test , columns = self . targets ), os . path . join ( dataset_folder , \"model_y_test.parquet\" ), ) # Save dataset locally self . _save_data ( self . data , os . path . join ( dataset_folder , \"dataset.parquet\" ))","title":"_export_train_test_data()"},{"location":"references/model/pipeline/#haferml.model.pipeline.DataSetX._save_data","text":"_save_data saves the dataframe locally. Parameters: Name Type Description Default dataframe pandas.DataFrame dataframe to be saved required destination str where the data is saved required Source code in haferml/model/pipeline.py @staticmethod def _save_data ( dataframe , destination ): \"\"\" `_save_data` saves the dataframe locally. :param dataframe: dataframe to be saved :type dataframe: pandas.DataFrame :param destination: where the data is saved :type destination: str \"\"\" logger . info ( \"Export test and train data\" ) dataframe . to_parquet ( destination )","title":"_save_data()"},{"location":"references/model/pipeline/#haferml.model.pipeline.DataSetX.create_train_test_datasets","text":"create_train_test_datasets will create self.data: the full input data right before train test split self.X_train, self.y_train, self.X_test, self.y_test Parameters: Name Type Description Default dataframe pandas.DataFrame the dataframe to be splitted. required Source code in haferml/model/pipeline.py def create_train_test_datasets ( self , dataframe ): \"\"\" create_train_test_datasets will create - self.data: the full input data right before train test split - self.X_train, self.y_train, self.X_test, self.y_test :param dataframe: the dataframe to be splitted. :type dataframe: pandas.DataFrame \"\"\" raise Exception ( \"create_train_test_dataset has not yet been implemented!\" )","title":"create_train_test_datasets()"},{"location":"references/model/pipeline/#haferml.model.pipeline.ModelSetX","text":"The core of the model including hyperparameters. Parameters: Name Type Description Default config dict a dictionary that contains the configurations. required base_folder str working directory where all the artifacts are being perserved. required","title":"ModelSetX"},{"location":"references/model/pipeline/#haferml.model.pipeline.ModelSetX.hyperparameters","text":"hyperparameters specifies the hyperparameters. This is a property.","title":"hyperparameters"},{"location":"references/model/pipeline/#haferml.model.pipeline.ModelSetX._set_hyperparameters","text":"_set_hyperparameters creates hyperpamater grid Source code in haferml/model/pipeline.py def _set_hyperparameters ( self ): \"\"\" _set_hyperparameters creates hyperpamater grid \"\"\" ...","title":"_set_hyperparameters()"},{"location":"references/model/pipeline/#haferml.model.pipeline.ModelSetX.create_model","text":"create_model creates the model and updates the property self.model . Source code in haferml/model/pipeline.py def create_model ( self ): ...","title":"create_model()"},{"location":"references/model/pipeline/#haferml.model.pipeline.ModelWorkflowX","text":"ModelWorkflowX class that holds DataSetX and ModelSetX Parameters: Name Type Description Default config dict a dictionary that contains the configs. required dataset haferml.model.DataSet a DataSet object that contains the data and provides a create_train_test_datasets method. required modelset haferml.model.ModelSet a ModelSet object that contains the model as well as the hyperparameters and a create_model . required base_folder str working directory where all the artifacts are being perserved. required","title":"ModelWorkflowX"},{"location":"references/model/pipeline/#haferml.model.pipeline.ModelWorkflowX.export_results","text":"export_results saves the necessary artifacts Source code in haferml/model/pipeline.py def export_results ( self ): \"\"\" export_results saves the necessary artifacts \"\"\" model_artifacts = self . artifacts [ \"model\" ] model_folder = model_artifacts [ \"local_absolute\" ] model_path = model_artifacts [ \"name_absolute\" ] if not os . path . exists ( model_folder ): os . makedirs ( model_folder ) logger . info ( \"Preserving models ...\" ) joblib . dump ( self . ModelSet . model , model_path ) logger . info ( \"Perserving logs ...\" ) log_file_path = f \" { model_path } .log\" logger . info ( f \"Save log file to { log_file_path } \" ) with open ( log_file_path , \"a+\" ) as fp : json . dump ( self . report , fp , default = isoencode ) fp . write ( \" \\n \" ) logger . info ( f \"Saved logs\" )","title":"export_results()"},{"location":"references/model/pipeline/#haferml.model.pipeline.ModelWorkflowX.fit_and_report","text":"_fit_and_report fits the model using input data and generate reports Source code in haferml/model/pipeline.py def fit_and_report ( self ): \"\"\" _fit_and_report fits the model using input data and generate reports \"\"\" logger . info ( \"Fitting the model ...\" ) logger . debug ( \"Shape of train data: \\n \" f \"X_train: { self . DataSet . X_train . shape } , { self . DataSet . X_train . sample ( 3 ) } \\n \" f \"y_train: { self . DataSet . y_train . shape } , { self . DataSet . y_train . sample ( 3 ) } \" ) self . ModelSet . model . fit ( self . DataSet . X_train . squeeze (), self . DataSet . y_train . squeeze () ) self . report = { \"hyperparameters\" : self . ModelSet . hyperparameters , \"best_params\" : self . ModelSet . model . best_params_ , \"cv_results\" : self . ModelSet . model . cv_results_ , } logger . debug ( self . report )","title":"fit_and_report()"},{"location":"references/model/pipeline/#haferml.model.pipeline.ModelWorkflowX.train","text":"train connects the training workflow Parameters: Name Type Description Default dataset pandas.DataFrame dataframe being used to train the model required Source code in haferml/model/pipeline.py def train ( self , dataset ): \"\"\" train connects the training workflow :param dataset: dataframe being used to train the model :type dataset: pandas.DataFrame \"\"\" logger . info ( \"1. Create train test datasets\" ) self . DataSet . create_train_test_datasets ( dataset ) logger . info ( \"2. Create model\" ) self . ModelSet . create_model () logger . info ( \"3. Fit model and report\" ) self . fit_and_report () logger . info ( \"4. Export results\" ) self . export_results ()","title":"train()"},{"location":"references/model/workflow/","text":"Model - Pipeline \u00a4 ModelWorkflow \u00a4 ModelWorkflow class that holds DataSet and ModelSet. Warning This class is a proposed framework. There are many member functions to be implemented if you are using the ModelWorkflow class. In most projects, we do not need to use this class. We have a class ModelWorkflowX which is a more detailed implementation in our model.pipeline module. ModelWorkflow takes a few arguments to instantiate. To run the workflow, use the train method. Parameters: Name Type Description Default config dict a dictionary that contains the configs. required dataset haferml.model.DataSet a DataSet object that contains the data and provides a create_train_test_datasets method. required modelset haferml.model.ModelSet a ModelSet object that contains the model as well as the hyperparameters and a create_model . required base_folder str working directory where all the artifacts are being perserved. required export_results ( self ) \u00a4 export_results saves the necessary artifacts Warning Please implement this method. Source code in haferml/model/workflow.py def export_results ( self ): \"\"\" export_results saves the necessary artifacts !!! warning Please implement this method. \"\"\" ... fit_and_report ( self ) \u00a4 _fit_and_report fits the model using input data and generate reports. Warning Please implement this method. Source code in haferml/model/workflow.py def fit_and_report ( self ): \"\"\" `_fit_and_report` fits the model using input data and generate reports. !!! warning Please implement this method. \"\"\" ... train ( self , dataset ) \u00a4 train connects the training workflow and executes the workflow step by step. Source code in haferml/model/workflow.py def train ( self , dataset ): \"\"\" train connects the training workflow and executes the workflow step by step. \"\"\" logger . info ( \"1. Create train test datasets\" ) self . DataSet . create_train_test_datasets ( dataset ) logger . info ( \"2. Create model\" ) self . ModelSet . create_model () logger . info ( \"3. Fit model and report\" ) self . fit_and_report () logger . info ( \"4. Export results\" ) self . export_results ()","title":"model.workflow"},{"location":"references/model/workflow/#model-pipeline","text":"","title":"Model - Pipeline"},{"location":"references/model/workflow/#haferml.model.workflow.ModelWorkflow","text":"ModelWorkflow class that holds DataSet and ModelSet. Warning This class is a proposed framework. There are many member functions to be implemented if you are using the ModelWorkflow class. In most projects, we do not need to use this class. We have a class ModelWorkflowX which is a more detailed implementation in our model.pipeline module. ModelWorkflow takes a few arguments to instantiate. To run the workflow, use the train method. Parameters: Name Type Description Default config dict a dictionary that contains the configs. required dataset haferml.model.DataSet a DataSet object that contains the data and provides a create_train_test_datasets method. required modelset haferml.model.ModelSet a ModelSet object that contains the model as well as the hyperparameters and a create_model . required base_folder str working directory where all the artifacts are being perserved. required","title":"ModelWorkflow"},{"location":"references/model/workflow/#haferml.model.workflow.ModelWorkflow.export_results","text":"export_results saves the necessary artifacts Warning Please implement this method. Source code in haferml/model/workflow.py def export_results ( self ): \"\"\" export_results saves the necessary artifacts !!! warning Please implement this method. \"\"\" ...","title":"export_results()"},{"location":"references/model/workflow/#haferml.model.workflow.ModelWorkflow.fit_and_report","text":"_fit_and_report fits the model using input data and generate reports. Warning Please implement this method. Source code in haferml/model/workflow.py def fit_and_report ( self ): \"\"\" `_fit_and_report` fits the model using input data and generate reports. !!! warning Please implement this method. \"\"\" ...","title":"fit_and_report()"},{"location":"references/model/workflow/#haferml.model.workflow.ModelWorkflow.train","text":"train connects the training workflow and executes the workflow step by step. Source code in haferml/model/workflow.py def train ( self , dataset ): \"\"\" train connects the training workflow and executes the workflow step by step. \"\"\" logger . info ( \"1. Create train test datasets\" ) self . DataSet . create_train_test_datasets ( dataset ) logger . info ( \"2. Create model\" ) self . ModelSet . create_model () logger . info ( \"3. Fit model and report\" ) self . fit_and_report () logger . info ( \"4. Export results\" ) self . export_results ()","title":"train()"},{"location":"references/preprocess/","text":"Preprocess \u00a4 haferml.preprocess module contains the utilities for preprocessing.","title":"Index"},{"location":"references/preprocess/#preprocess","text":"haferml.preprocess module contains the utilities for preprocessing.","title":"Preprocess"},{"location":"references/preprocess/ingredients/","text":"Preprocess - Ingredients \u00a4 OrderedProcessor \u00a4 Go through an ordered methods in OrderedProcessor to transform a dataframe. Beware of the exceptions. Add member functions to transform the dataframe @attribute(order=1) def _transformer_created_at(self, dataframe): pass _get_transforms ( self ) private \u00a4 _get_transforms extracts the list of transformers. This method can be replaced by the decorator with_transforms . Source code in haferml/preprocess/ingredients.py def _get_transforms ( self ): \"\"\" _get_transforms extracts the list of transformers. This method can be replaced by the decorator `with_transforms`. \"\"\" all_methods = dict ( inspect . getmembers ( self )) transforms = [] for method_name , method_func in all_methods . items (): if hasattr ( method_func , \"order\" ): method_order = method_func . order logger . info ( f \" { method_name } has order { method_order } \" ) transforms . append ( { \"name\" : method_name , \"method\" : method_func , \"order\" : method_order } ) transforms = sorted ( transforms , key = lambda k : k [ \"order\" ]) self . transforms = { m [ \"name\" ]: m [ \"method\" ] for m in transforms } logger . debug ( \"All methods: {} \" . format ( all_methods )) logger . info ( \"Ordered predefined transformers: {} \" . format ( self . transforms )) attributes ( ** attrs ) \u00a4 A decorator to set attributes of member functions in a class. class AGoodClass : def __init__ ( self ): self . size = 0 @attributes ( order = 1 ) def first_good_member ( self , new ): return \"first good member\" @attributes ( order = 2 ) def second_good_member ( self , new ): return \"second good member\" References: 1. https://stackoverflow.com/a/48146924/1477359 Source code in haferml/preprocess/ingredients.py def attributes ( ** attrs ): \"\"\" A decorator to set attributes of member functions in a class. ```python class AGoodClass: def __init__(self): self.size = 0 @attributes(order=1) def first_good_member(self, new): return \"first good member\" @attributes(order=2) def second_good_member(self, new): return \"second good member\" ``` References: 1. https://stackoverflow.com/a/48146924/1477359 \"\"\" def decorator ( f ): @wraps ( f ) def wrapper ( * args , ** kwargs ): return f ( * args , ** kwargs ) for attr_name , attr_value in attrs . items (): setattr ( wrapper , attr_name , attr_value ) return wrapper return decorator order ( ord ) \u00a4 order is decorator to order the pipeline classes. This decorator specifies a property named \"order\" to the member function so that we can use the property to order the member functions. This order function can be combined with the decorator with_transforms which orders the member functions. class AGoodClass : def __init__ ( self ): self . size = 0 @order ( 1 ) def first_good_member ( self , new ): return \"first good member\" @order ( 2 ) def second_good_member ( self , new ): return \"second good member\" Source code in haferml/preprocess/ingredients.py def order ( ord ): \"\"\" `order` is decorator to order the pipeline classes. This decorator specifies a property named \"order\" to the member function so that we can use the property to order the member functions. This `order` function can be combined with the decorator `with_transforms` which orders the member functions. ```python class AGoodClass: def __init__(self): self.size = 0 @order(1) def first_good_member(self, new): return \"first good member\" @order(2) def second_good_member(self, new): return \"second good member\" ``` \"\"\" return attributes ( order = ord ) with_transforms ( attr = None ) \u00a4 with_transforms is a decorator that builds the ordered transformations and assigns the transforms to the property self.transforms . self.transforms is a dictionary and the keys are the name of the transformation functions. @with_transforms def run ( self ): for t in self . transforms : dataframe = self . transforms [ t ]( dataframe ) logger . info ( f \"transformation { t } is done.\" ) Here is a full example of the decorator. class AGoodClass : def __init__ ( self ): pass @order ( 1 ) def first_good_member ( self , new ): return f \" { new } - appended first good member\" @order ( 2 ) def second_good_member ( self , new ): return f \" { new } - appended second good member\" @with_transforms () def bench ( self , name ): logger . info ( name ) logger . info ( self . transforms ) for t in self . transforms : name = self . transforms [ t ]( name ) logger . info ( f \"transformation { t } is done. Got strings: { name } \" ) a = AGoodClass () a . bench ( \"a name\" ) If you would rather use a different attribute name such as \"rank\", the with_transforms decorator can also be customized. class AGoodClass : def __init__ ( self ): pass @attributes ( rank = 1 ) def first_good_member ( self , new ): return f \" { new } - appended first good member\" @attributes ( rank = 2 ) def second_good_member ( self , new ): return f \" { new } - appended second good member\" @with_transforms ( attr = \"rank\" ) def bench ( self , name ): logger . info ( name ) logger . info ( self . transforms ) for t in self . transforms : name = self . transforms [ t ]( name ) logger . info ( f \"transformation { t } is done. Got strings: { name } \" ) a = AGoodClass () a . bench ( \"a name\" ) Source code in haferml/preprocess/ingredients.py def with_transforms ( attr = None ): \"\"\" with_transforms is a decorator that builds the ordered transformations and assigns the transforms to the property `self.transforms`. `self.transforms` is a dictionary and the keys are the name of the transformation functions. ```python @with_transforms def run(self): for t in self.transforms: dataframe = self.transforms[t](dataframe) logger.info(f\"transformation {t} is done.\") ``` Here is a full example of the decorator. ```python class AGoodClass: def __init__(self): pass @order(1) def first_good_member(self, new): return f\"{new} - appended first good member\" @order(2) def second_good_member(self, new): return f\"{new} - appended second good member\" @with_transforms() def bench(self, name): logger.info(name) logger.info(self.transforms) for t in self.transforms: name = self.transforms[t](name) logger.info(f\"transformation {t} is done. Got strings: {name}\") a = AGoodClass() a.bench(\"a name\") ``` If you would rather use a different attribute name such as \"rank\", the `with_transforms` decorator can also be customized. ```python class AGoodClass: def __init__(self): pass @attributes(rank=1) def first_good_member(self, new): return f\"{new} - appended first good member\" @attributes(rank=2) def second_good_member(self, new): return f\"{new} - appended second good member\" @with_transforms(attr=\"rank\") def bench(self, name): logger.info(name) logger.info(self.transforms) for t in self.transforms: name = self.transforms[t](name) logger.info(f\"transformation {t} is done. Got strings: {name}\") a = AGoodClass() a.bench(\"a name\") ``` \"\"\" if attr is None : attr = \"order\" def _with_transforms ( f ): def _get_transforms ( self , * args ): all_methods = dict ( inspect . getmembers ( self )) transforms = [] for method_name , method_func in all_methods . items (): if hasattr ( method_func , attr ): method_order = getattr ( method_func , attr ) logger . debug ( f \" { method_name } has order { method_order } \" ) transforms . append ( { \"name\" : method_name , \"method\" : method_func , attr : method_order } ) transforms = sorted ( transforms , key = lambda k : k [ attr ]) self . transforms = { m [ \"name\" ]: m [ \"method\" ] for m in transforms } logger . debug ( \"All methods: {} \" . format ( all_methods )) logger . debug ( \"Ordered predefined transformers: {} \" . format ( self . transforms )) return f ( self , * args ) return _get_transforms return _with_transforms","title":"preprocess.ingredients"},{"location":"references/preprocess/ingredients/#preprocess-ingredients","text":"","title":"Preprocess - Ingredients"},{"location":"references/preprocess/ingredients/#haferml.preprocess.ingredients.OrderedProcessor","text":"Go through an ordered methods in OrderedProcessor to transform a dataframe. Beware of the exceptions. Add member functions to transform the dataframe @attribute(order=1) def _transformer_created_at(self, dataframe): pass","title":"OrderedProcessor"},{"location":"references/preprocess/ingredients/#haferml.preprocess.ingredients.OrderedProcessor._get_transforms","text":"_get_transforms extracts the list of transformers. This method can be replaced by the decorator with_transforms . Source code in haferml/preprocess/ingredients.py def _get_transforms ( self ): \"\"\" _get_transforms extracts the list of transformers. This method can be replaced by the decorator `with_transforms`. \"\"\" all_methods = dict ( inspect . getmembers ( self )) transforms = [] for method_name , method_func in all_methods . items (): if hasattr ( method_func , \"order\" ): method_order = method_func . order logger . info ( f \" { method_name } has order { method_order } \" ) transforms . append ( { \"name\" : method_name , \"method\" : method_func , \"order\" : method_order } ) transforms = sorted ( transforms , key = lambda k : k [ \"order\" ]) self . transforms = { m [ \"name\" ]: m [ \"method\" ] for m in transforms } logger . debug ( \"All methods: {} \" . format ( all_methods )) logger . info ( \"Ordered predefined transformers: {} \" . format ( self . transforms ))","title":"_get_transforms()"},{"location":"references/preprocess/ingredients/#haferml.preprocess.ingredients.attributes","text":"A decorator to set attributes of member functions in a class. class AGoodClass : def __init__ ( self ): self . size = 0 @attributes ( order = 1 ) def first_good_member ( self , new ): return \"first good member\" @attributes ( order = 2 ) def second_good_member ( self , new ): return \"second good member\" References: 1. https://stackoverflow.com/a/48146924/1477359 Source code in haferml/preprocess/ingredients.py def attributes ( ** attrs ): \"\"\" A decorator to set attributes of member functions in a class. ```python class AGoodClass: def __init__(self): self.size = 0 @attributes(order=1) def first_good_member(self, new): return \"first good member\" @attributes(order=2) def second_good_member(self, new): return \"second good member\" ``` References: 1. https://stackoverflow.com/a/48146924/1477359 \"\"\" def decorator ( f ): @wraps ( f ) def wrapper ( * args , ** kwargs ): return f ( * args , ** kwargs ) for attr_name , attr_value in attrs . items (): setattr ( wrapper , attr_name , attr_value ) return wrapper return decorator","title":"attributes()"},{"location":"references/preprocess/ingredients/#haferml.preprocess.ingredients.order","text":"order is decorator to order the pipeline classes. This decorator specifies a property named \"order\" to the member function so that we can use the property to order the member functions. This order function can be combined with the decorator with_transforms which orders the member functions. class AGoodClass : def __init__ ( self ): self . size = 0 @order ( 1 ) def first_good_member ( self , new ): return \"first good member\" @order ( 2 ) def second_good_member ( self , new ): return \"second good member\" Source code in haferml/preprocess/ingredients.py def order ( ord ): \"\"\" `order` is decorator to order the pipeline classes. This decorator specifies a property named \"order\" to the member function so that we can use the property to order the member functions. This `order` function can be combined with the decorator `with_transforms` which orders the member functions. ```python class AGoodClass: def __init__(self): self.size = 0 @order(1) def first_good_member(self, new): return \"first good member\" @order(2) def second_good_member(self, new): return \"second good member\" ``` \"\"\" return attributes ( order = ord )","title":"order()"},{"location":"references/preprocess/ingredients/#haferml.preprocess.ingredients.with_transforms","text":"with_transforms is a decorator that builds the ordered transformations and assigns the transforms to the property self.transforms . self.transforms is a dictionary and the keys are the name of the transformation functions. @with_transforms def run ( self ): for t in self . transforms : dataframe = self . transforms [ t ]( dataframe ) logger . info ( f \"transformation { t } is done.\" ) Here is a full example of the decorator. class AGoodClass : def __init__ ( self ): pass @order ( 1 ) def first_good_member ( self , new ): return f \" { new } - appended first good member\" @order ( 2 ) def second_good_member ( self , new ): return f \" { new } - appended second good member\" @with_transforms () def bench ( self , name ): logger . info ( name ) logger . info ( self . transforms ) for t in self . transforms : name = self . transforms [ t ]( name ) logger . info ( f \"transformation { t } is done. Got strings: { name } \" ) a = AGoodClass () a . bench ( \"a name\" ) If you would rather use a different attribute name such as \"rank\", the with_transforms decorator can also be customized. class AGoodClass : def __init__ ( self ): pass @attributes ( rank = 1 ) def first_good_member ( self , new ): return f \" { new } - appended first good member\" @attributes ( rank = 2 ) def second_good_member ( self , new ): return f \" { new } - appended second good member\" @with_transforms ( attr = \"rank\" ) def bench ( self , name ): logger . info ( name ) logger . info ( self . transforms ) for t in self . transforms : name = self . transforms [ t ]( name ) logger . info ( f \"transformation { t } is done. Got strings: { name } \" ) a = AGoodClass () a . bench ( \"a name\" ) Source code in haferml/preprocess/ingredients.py def with_transforms ( attr = None ): \"\"\" with_transforms is a decorator that builds the ordered transformations and assigns the transforms to the property `self.transforms`. `self.transforms` is a dictionary and the keys are the name of the transformation functions. ```python @with_transforms def run(self): for t in self.transforms: dataframe = self.transforms[t](dataframe) logger.info(f\"transformation {t} is done.\") ``` Here is a full example of the decorator. ```python class AGoodClass: def __init__(self): pass @order(1) def first_good_member(self, new): return f\"{new} - appended first good member\" @order(2) def second_good_member(self, new): return f\"{new} - appended second good member\" @with_transforms() def bench(self, name): logger.info(name) logger.info(self.transforms) for t in self.transforms: name = self.transforms[t](name) logger.info(f\"transformation {t} is done. Got strings: {name}\") a = AGoodClass() a.bench(\"a name\") ``` If you would rather use a different attribute name such as \"rank\", the `with_transforms` decorator can also be customized. ```python class AGoodClass: def __init__(self): pass @attributes(rank=1) def first_good_member(self, new): return f\"{new} - appended first good member\" @attributes(rank=2) def second_good_member(self, new): return f\"{new} - appended second good member\" @with_transforms(attr=\"rank\") def bench(self, name): logger.info(name) logger.info(self.transforms) for t in self.transforms: name = self.transforms[t](name) logger.info(f\"transformation {t} is done. Got strings: {name}\") a = AGoodClass() a.bench(\"a name\") ``` \"\"\" if attr is None : attr = \"order\" def _with_transforms ( f ): def _get_transforms ( self , * args ): all_methods = dict ( inspect . getmembers ( self )) transforms = [] for method_name , method_func in all_methods . items (): if hasattr ( method_func , attr ): method_order = getattr ( method_func , attr ) logger . debug ( f \" { method_name } has order { method_order } \" ) transforms . append ( { \"name\" : method_name , \"method\" : method_func , attr : method_order } ) transforms = sorted ( transforms , key = lambda k : k [ attr ]) self . transforms = { m [ \"name\" ]: m [ \"method\" ] for m in transforms } logger . debug ( \"All methods: {} \" . format ( all_methods )) logger . debug ( \"Ordered predefined transformers: {} \" . format ( self . transforms )) return f ( self , * args ) return _get_transforms return _with_transforms","title":"with_transforms()"},{"location":"references/preprocess/pipeline/","text":"Preprocess - Pipeline \u00a4 BasePreProcessor ( OrderedProcessor ) \u00a4 Shared methods to transform the datasets The following example demonstrates how to use it. from haferml.preprocess.ingredients import BaseProcessor , attributes class DemoPreProcessor ( BasePreProcessor ): def __init__ ( self , config , columns , cutoff_timestamp = None ): super ( DemoPreProcessor , self ) . __init__ ( config = config , columns = columns ) self . cutoff_timestamp = cutoff_timestamp def merge_datasets ( self , datasets ): df_a = datasets [ \"a\" ] df_b = datasets [ \"b\" ] # 1. We only take data that is later than a certain date if self . cutoff_timestamp : filter_a_mask = ( df_a . req_created_at > self . cutoff_timestamp ) df_a = df_a . loc [ filter_a_mask ] # combine dataset a and b dataset = pd . merge ( df_a , df_b , how = \"left\" , on = \"request_id\" , ) return dataset @attributes ( order = 1 ) def _fix_names ( self , dataset ): dataset [ \"names\" ] = dataset . names . replace ( \"Tima\" , \"Tim\" ) @attributes ( order = 2 ) def _convert_requirement_to_bool ( self , dataset ): dataset [ \"requirements\" ] = dataset . requirements . apply ( lambda x : False if pd . isnull ( x ) else True ) @attributes ( order = 12 ) def _filter_columns_and_crossing ( self , dataset ): # _filter_columns_and_crossing removes unnecessary columns and append crossings # only keep the specified columns # the following code also deals with feature crossings if self . columns : columns = list ( set ( self . columns )) crossing = [] features_ex = [] for col in columns : if \"__\" in col : crossing . append ( col ) else : features_ex . append ( col ) dataset = dataset [ features_ex ] for fc in crossing : fc_cols = fc . split ( \"__\" ) fc_series = dataset [ fc_cols [ 0 ]] for fc_col in fc_cols [ 1 :]: fc_series = fc_series * dataset [ fc_col ] dataset [ fc ] = fc_series dp = DemoPreProcessor ( config = {}, columns = [ 'names' , 'requirements' , 'names__requirements' ]) dataset = { \"a\" : pd . DataFrame ([{ \"names\" : \"Tima Cook\" , \"requirements\" : \"I need it\" }]), \"b\" : pd . DataFrame ([{ \"names\" : \"Time Cook\" , \"requirements\" : None }]) } dp . run ( dataset ) merge_datasets ( self , datasets ) \u00a4 merge_datasets merges the datasets into one singular dataframe to be processed. Parameters: Name Type Description Default datasets dict dictionary that contains the dataframes required Source code in haferml/preprocess/pipeline.py def merge_datasets ( self , datasets ): \"\"\" merge_datasets merges the datasets into one singular dataframe to be processed. :param datasets: dictionary that contains the dataframes :type datasets: dict \"\"\" raise NotImplementedError ( \"Please implement this method!\" ) run ( self , datasets , ** params ) \u00a4 run connects the transforms into pipelines Parameters: Name Type Description Default datasets input datasets as list or dict of single dataframe required params for example, merge=True can be used to indicate whether and how to merge datasets which will run the method merge_datasets(datasets) {} Source code in haferml/preprocess/pipeline.py def run ( self , datasets , ** params ): \"\"\" run connects the transforms into pipelines :param datasets: input datasets as list or dict of single dataframe :param params: for example, `merge=True` can be used to indicate whether and how to merge datasets which will run the method `merge_datasets(datasets)` \"\"\" if params . get ( \"merge\" ) is True : dataframe = self . merge_datasets ( datasets ) elif isinstance ( datasets , dict ): logger . warning ( \"No specific merge methods specified \\n \" \"Auto concating all the datasets\" ) dataframe = pd . concat ( datasets . values ()) elif isinstance ( datasets , list ): logger . warning ( \"No specific merge methods specified \\n \" \"Auto concating all the datasets\" ) dataframe = pd . concat ( datasets ) elif isinstance ( datasets , pd . DataFrame ): logger . info ( \"Input dataset is a single dataframe, \" \"making a copy for safety\" ) dataframe = datasets . copy () else : raise TypeError ( \"Input datasets should be dataframe or list of dataframes\" ) # Go through the transforms for t in self . transforms : logger . info ( f \"Performing { t } ...\" ) dataframe = self . transforms [ t ]( dataframe ) logger . info ( f \" { t } is done.\" ) return dataframe","title":"preprocess.pipeline"},{"location":"references/preprocess/pipeline/#preprocess-pipeline","text":"","title":"Preprocess - Pipeline"},{"location":"references/preprocess/pipeline/#haferml.preprocess.pipeline.BasePreProcessor","text":"Shared methods to transform the datasets The following example demonstrates how to use it. from haferml.preprocess.ingredients import BaseProcessor , attributes class DemoPreProcessor ( BasePreProcessor ): def __init__ ( self , config , columns , cutoff_timestamp = None ): super ( DemoPreProcessor , self ) . __init__ ( config = config , columns = columns ) self . cutoff_timestamp = cutoff_timestamp def merge_datasets ( self , datasets ): df_a = datasets [ \"a\" ] df_b = datasets [ \"b\" ] # 1. We only take data that is later than a certain date if self . cutoff_timestamp : filter_a_mask = ( df_a . req_created_at > self . cutoff_timestamp ) df_a = df_a . loc [ filter_a_mask ] # combine dataset a and b dataset = pd . merge ( df_a , df_b , how = \"left\" , on = \"request_id\" , ) return dataset @attributes ( order = 1 ) def _fix_names ( self , dataset ): dataset [ \"names\" ] = dataset . names . replace ( \"Tima\" , \"Tim\" ) @attributes ( order = 2 ) def _convert_requirement_to_bool ( self , dataset ): dataset [ \"requirements\" ] = dataset . requirements . apply ( lambda x : False if pd . isnull ( x ) else True ) @attributes ( order = 12 ) def _filter_columns_and_crossing ( self , dataset ): # _filter_columns_and_crossing removes unnecessary columns and append crossings # only keep the specified columns # the following code also deals with feature crossings if self . columns : columns = list ( set ( self . columns )) crossing = [] features_ex = [] for col in columns : if \"__\" in col : crossing . append ( col ) else : features_ex . append ( col ) dataset = dataset [ features_ex ] for fc in crossing : fc_cols = fc . split ( \"__\" ) fc_series = dataset [ fc_cols [ 0 ]] for fc_col in fc_cols [ 1 :]: fc_series = fc_series * dataset [ fc_col ] dataset [ fc ] = fc_series dp = DemoPreProcessor ( config = {}, columns = [ 'names' , 'requirements' , 'names__requirements' ]) dataset = { \"a\" : pd . DataFrame ([{ \"names\" : \"Tima Cook\" , \"requirements\" : \"I need it\" }]), \"b\" : pd . DataFrame ([{ \"names\" : \"Time Cook\" , \"requirements\" : None }]) } dp . run ( dataset )","title":"BasePreProcessor"},{"location":"references/preprocess/pipeline/#haferml.preprocess.pipeline.BasePreProcessor.merge_datasets","text":"merge_datasets merges the datasets into one singular dataframe to be processed. Parameters: Name Type Description Default datasets dict dictionary that contains the dataframes required Source code in haferml/preprocess/pipeline.py def merge_datasets ( self , datasets ): \"\"\" merge_datasets merges the datasets into one singular dataframe to be processed. :param datasets: dictionary that contains the dataframes :type datasets: dict \"\"\" raise NotImplementedError ( \"Please implement this method!\" )","title":"merge_datasets()"},{"location":"references/preprocess/pipeline/#haferml.preprocess.pipeline.BasePreProcessor.run","text":"run connects the transforms into pipelines Parameters: Name Type Description Default datasets input datasets as list or dict of single dataframe required params for example, merge=True can be used to indicate whether and how to merge datasets which will run the method merge_datasets(datasets) {} Source code in haferml/preprocess/pipeline.py def run ( self , datasets , ** params ): \"\"\" run connects the transforms into pipelines :param datasets: input datasets as list or dict of single dataframe :param params: for example, `merge=True` can be used to indicate whether and how to merge datasets which will run the method `merge_datasets(datasets)` \"\"\" if params . get ( \"merge\" ) is True : dataframe = self . merge_datasets ( datasets ) elif isinstance ( datasets , dict ): logger . warning ( \"No specific merge methods specified \\n \" \"Auto concating all the datasets\" ) dataframe = pd . concat ( datasets . values ()) elif isinstance ( datasets , list ): logger . warning ( \"No specific merge methods specified \\n \" \"Auto concating all the datasets\" ) dataframe = pd . concat ( datasets ) elif isinstance ( datasets , pd . DataFrame ): logger . info ( \"Input dataset is a single dataframe, \" \"making a copy for safety\" ) dataframe = datasets . copy () else : raise TypeError ( \"Input datasets should be dataframe or list of dataframes\" ) # Go through the transforms for t in self . transforms : logger . info ( f \"Performing { t } ...\" ) dataframe = self . transforms [ t ]( dataframe ) logger . info ( f \" { t } is done.\" ) return dataframe","title":"run()"},{"location":"references/sync/","text":"Sync \u00a4 haferml.sync module contains the utilities for syncing artifacts.","title":"Index"},{"location":"references/sync/#sync","text":"haferml.sync module contains the utilities for syncing artifacts.","title":"Sync"},{"location":"references/sync/aws/","text":"Sync - AWS \u00a4 aws_cli ( * cmd ) \u00a4 aws_cli invokes the aws cli processes in python to execute awscli commands. Warning This is not the most elegant way of using awscli. However, it has been a convinient function in data science projects. This function is adapted from https://github.com/boto/boto3/issues/358#issuecomment-372086466 AWS credential env variables should be configured before calling this function. The awscli command should be wrapped as a tuple. To download data from S3 to a local path, use >>> aws_cli(('s3', 'sync', 's3://s2-fpd/augmentation/', '/tmp/test')) Similarly, upload is done in the following way >>> # local_path = '' >>> # remote_path = '' >>> _aws_cli(('s3', 'sync', local_path, remote_path)) Parameters: Name Type Description Default *cmd tuple of awscli command. () Source code in haferml/sync/aws.py def aws_cli ( * cmd ): \"\"\" aws_cli invokes the aws cli processes in python to execute awscli commands. !!! warning This is not the most elegant way of using awscli. However, it has been a convinient function in data science projects. This function is adapted from https://github.com/boto/boto3/issues/358#issuecomment-372086466 AWS credential env variables should be configured before calling this function. The awscli command should be wrapped as a tuple. To download data from S3 to a local path, use ``` >>> aws_cli(('s3', 'sync', 's3://s2-fpd/augmentation/', '/tmp/test')) Similarly, upload is done in the following way >>> # local_path = '' >>> # remote_path = '' >>> _aws_cli(('s3', 'sync', local_path, remote_path)) ``` :param *cmd: tuple of awscli command. \"\"\" old_env = dict ( os . environ ) try : # Set up environment env = os . environ . copy () env [ \"LC_CTYPE\" ] = \"en_US.UTF\" os . environ . update ( env ) # Run awscli in the same process exit_code = create_clidriver () . main ( * cmd ) # Deal with problems if exit_code > 0 : raise RuntimeError ( f \"AWS CLI exited with code { exit_code } \" ) finally : os . environ . clear () os . environ . update ( old_env ) s3_download ( path , folder ) \u00a4 s3_download downloads files from S3. >>> s3_download(config_path, base_folder) Parameters: Name Type Description Default path str s3 uri required folder str destination folder required Source code in haferml/sync/aws.py def s3_download ( path , folder ): \"\"\" `s3_download` downloads files from S3. ``` >>> s3_download(config_path, base_folder) ``` :param path: s3 uri :type path: str :param folder: destination folder :type folder: str \"\"\" if not path . startswith ( \"s3://\" ): raise Exception ( f \" { path } is not S3 uri!\" ) else : # e.g., s3://mein-work/abc/performance/model_performance_log.json s3_bucket = path . split ( \"/\" )[ 2 ] s3_filepath = \"/\" . join ( path . split ( \"/\" )[ 3 :]) # get the name of the config file s3_filename = path . split ( \"/\" )[ - 1 ] # local config path is constructed from base folder and filename path = os . path . join ( folder , s3_filename ) s3 = boto3 . client ( \"s3\" ) s3 . download_file ( s3_bucket , s3_filepath , path )","title":"sync.aws"},{"location":"references/sync/aws/#sync-aws","text":"","title":"Sync - AWS"},{"location":"references/sync/aws/#haferml.sync.aws.aws_cli","text":"aws_cli invokes the aws cli processes in python to execute awscli commands. Warning This is not the most elegant way of using awscli. However, it has been a convinient function in data science projects. This function is adapted from https://github.com/boto/boto3/issues/358#issuecomment-372086466 AWS credential env variables should be configured before calling this function. The awscli command should be wrapped as a tuple. To download data from S3 to a local path, use >>> aws_cli(('s3', 'sync', 's3://s2-fpd/augmentation/', '/tmp/test')) Similarly, upload is done in the following way >>> # local_path = '' >>> # remote_path = '' >>> _aws_cli(('s3', 'sync', local_path, remote_path)) Parameters: Name Type Description Default *cmd tuple of awscli command. () Source code in haferml/sync/aws.py def aws_cli ( * cmd ): \"\"\" aws_cli invokes the aws cli processes in python to execute awscli commands. !!! warning This is not the most elegant way of using awscli. However, it has been a convinient function in data science projects. This function is adapted from https://github.com/boto/boto3/issues/358#issuecomment-372086466 AWS credential env variables should be configured before calling this function. The awscli command should be wrapped as a tuple. To download data from S3 to a local path, use ``` >>> aws_cli(('s3', 'sync', 's3://s2-fpd/augmentation/', '/tmp/test')) Similarly, upload is done in the following way >>> # local_path = '' >>> # remote_path = '' >>> _aws_cli(('s3', 'sync', local_path, remote_path)) ``` :param *cmd: tuple of awscli command. \"\"\" old_env = dict ( os . environ ) try : # Set up environment env = os . environ . copy () env [ \"LC_CTYPE\" ] = \"en_US.UTF\" os . environ . update ( env ) # Run awscli in the same process exit_code = create_clidriver () . main ( * cmd ) # Deal with problems if exit_code > 0 : raise RuntimeError ( f \"AWS CLI exited with code { exit_code } \" ) finally : os . environ . clear () os . environ . update ( old_env )","title":"aws_cli()"},{"location":"references/sync/aws/#haferml.sync.aws.s3_download","text":"s3_download downloads files from S3. >>> s3_download(config_path, base_folder) Parameters: Name Type Description Default path str s3 uri required folder str destination folder required Source code in haferml/sync/aws.py def s3_download ( path , folder ): \"\"\" `s3_download` downloads files from S3. ``` >>> s3_download(config_path, base_folder) ``` :param path: s3 uri :type path: str :param folder: destination folder :type folder: str \"\"\" if not path . startswith ( \"s3://\" ): raise Exception ( f \" { path } is not S3 uri!\" ) else : # e.g., s3://mein-work/abc/performance/model_performance_log.json s3_bucket = path . split ( \"/\" )[ 2 ] s3_filepath = \"/\" . join ( path . split ( \"/\" )[ 3 :]) # get the name of the config file s3_filename = path . split ( \"/\" )[ - 1 ] # local config path is constructed from base folder and filename path = os . path . join ( folder , s3_filename ) s3 = boto3 . client ( \"s3\" ) s3 . download_file ( s3_bucket , s3_filepath , path )","title":"s3_download()"},{"location":"references/sync/local/","text":"Sync - Local \u00a4 isoencode ( obj ) \u00a4 isoencode decodes many different objects such as np.bool -> regular bool. with open ( log_file_path , \"a+\" ) as fp : json . dump ( self . report , fp , default = isoencode ) Source code in haferml/sync/local.py def isoencode ( obj ): \"\"\" isoencode decodes many different objects such as np.bool -> regular bool. ```python with open(log_file_path, \"a+\") as fp: json.dump(self.report, fp, default=isoencode) ``` \"\"\" if isinstance ( obj , datetime . datetime ): return obj . isoformat () if isinstance ( obj , datetime . date ): return obj . isoformat () if isinstance ( obj , np . ndarray ): return obj . tolist () if isinstance ( obj , np . int64 ): return int ( obj ) if isinstance ( obj , np . float64 ): return float ( obj ) if isinstance ( obj , np . bool_ ): return bool ( obj ) load_records ( data_path_inp ) \u00a4 Load data from a line deliminated json file. Instead of loading pandas for such a simple job, this function does the work in most cases. Parameters: Name Type Description Default data_path_inp data file path required Returns: Type Description list of dicts Source code in haferml/sync/local.py def load_records ( data_path_inp ): \"\"\"Load data from a line deliminated json file. Instead of loading pandas for such a simple job, this function does the work in most cases. :param data_path_inp: data file path :return: list of dicts \"\"\" data = [] with open ( data_path_inp , \"r\" ) as fp : for line in fp : line = line . replace ( \"null\" , ' \"None\" ' ) try : line_data = json . loads ( line . strip ()) except Exception as ee : logger . warning ( \"could not load \" , line , \" \\n \" , ee ) data . append ( line_data ) return data prepare_folders ( folder_list = None , base_folder = None ) \u00a4 prepare_folders creates the necessary folders Parameters: Name Type Description Default base_folder str base folder of the whole project None folder_list list list of folders to create, relative to base_folder None Source code in haferml/sync/local.py def prepare_folders ( folder_list = None , base_folder = None ): \"\"\" prepare_folders creates the necessary folders :param base_folder: base folder of the whole project :type base_folder: str :param folder_list: list of folders to create, relative to base_folder :type folder_list: list \"\"\" if folder_list is None : raise Exception ( \"Please specify the list of folder using fodler_list\" ) if os . path . exists ( base_folder ): logger . info ( f \"Using base folder { base_folder } !\" ) # prepare the model folder if isinstance ( folder_list , ( tuple , list , set )): pass elif isinstance ( folder_list , str ): logger . warning ( f \"Converting to list: { folder_list } to a list\" ) folder_list = [ folder_list ] for folder in folder_list : folder = os . path . join ( base_folder , folder ) if not os . path . exists ( folder ): os . makedirs ( folder ) logger . info ( f \"created { folder } \" ) save_records ( data_inp , output , is_flush = None , write_mode = None ) \u00a4 Save list of dicts to file. Instead of loading pandas for such a simple job, this function does the work in most cases. :is_flush: whether to flush data to file for each row written to file Parameters: Name Type Description Default data_inp dict or list of dict to be saved required output path to output file required Returns: Type Description None Source code in haferml/sync/local.py def save_records ( data_inp , output , is_flush = None , write_mode = None ): \"\"\"Save list of dicts to file. Instead of loading pandas for such a simple job, this function does the work in most cases. :param data_inp: dict or list of dict to be saved :param output: path to output file :is_flush: whether to flush data to file for each row written to file :return: None \"\"\" if write_mode is None : write_mode = \"a+\" if is_flush is None : is_flush = False if isinstance ( data_inp , list ): data = data_inp elif isinstance ( data_inp , dict ): data = [ data_inp ] else : raise Exception ( \"Input data is neither list nor dict: {} \" . format ( data_inp )) try : with open ( output , write_mode ) as fp : for i in data : json . dump ( i , fp ) fp . write ( \" \\n \" ) if is_flush : fp . flush () except Exception as ee : raise Exception ( \"Could not load data to file: {} \" . format ( ee ))","title":"sync.local"},{"location":"references/sync/local/#sync-local","text":"","title":"Sync - Local"},{"location":"references/sync/local/#haferml.sync.local.isoencode","text":"isoencode decodes many different objects such as np.bool -> regular bool. with open ( log_file_path , \"a+\" ) as fp : json . dump ( self . report , fp , default = isoencode ) Source code in haferml/sync/local.py def isoencode ( obj ): \"\"\" isoencode decodes many different objects such as np.bool -> regular bool. ```python with open(log_file_path, \"a+\") as fp: json.dump(self.report, fp, default=isoencode) ``` \"\"\" if isinstance ( obj , datetime . datetime ): return obj . isoformat () if isinstance ( obj , datetime . date ): return obj . isoformat () if isinstance ( obj , np . ndarray ): return obj . tolist () if isinstance ( obj , np . int64 ): return int ( obj ) if isinstance ( obj , np . float64 ): return float ( obj ) if isinstance ( obj , np . bool_ ): return bool ( obj )","title":"isoencode()"},{"location":"references/sync/local/#haferml.sync.local.load_records","text":"Load data from a line deliminated json file. Instead of loading pandas for such a simple job, this function does the work in most cases. Parameters: Name Type Description Default data_path_inp data file path required Returns: Type Description list of dicts Source code in haferml/sync/local.py def load_records ( data_path_inp ): \"\"\"Load data from a line deliminated json file. Instead of loading pandas for such a simple job, this function does the work in most cases. :param data_path_inp: data file path :return: list of dicts \"\"\" data = [] with open ( data_path_inp , \"r\" ) as fp : for line in fp : line = line . replace ( \"null\" , ' \"None\" ' ) try : line_data = json . loads ( line . strip ()) except Exception as ee : logger . warning ( \"could not load \" , line , \" \\n \" , ee ) data . append ( line_data ) return data","title":"load_records()"},{"location":"references/sync/local/#haferml.sync.local.prepare_folders","text":"prepare_folders creates the necessary folders Parameters: Name Type Description Default base_folder str base folder of the whole project None folder_list list list of folders to create, relative to base_folder None Source code in haferml/sync/local.py def prepare_folders ( folder_list = None , base_folder = None ): \"\"\" prepare_folders creates the necessary folders :param base_folder: base folder of the whole project :type base_folder: str :param folder_list: list of folders to create, relative to base_folder :type folder_list: list \"\"\" if folder_list is None : raise Exception ( \"Please specify the list of folder using fodler_list\" ) if os . path . exists ( base_folder ): logger . info ( f \"Using base folder { base_folder } !\" ) # prepare the model folder if isinstance ( folder_list , ( tuple , list , set )): pass elif isinstance ( folder_list , str ): logger . warning ( f \"Converting to list: { folder_list } to a list\" ) folder_list = [ folder_list ] for folder in folder_list : folder = os . path . join ( base_folder , folder ) if not os . path . exists ( folder ): os . makedirs ( folder ) logger . info ( f \"created { folder } \" )","title":"prepare_folders()"},{"location":"references/sync/local/#haferml.sync.local.save_records","text":"Save list of dicts to file. Instead of loading pandas for such a simple job, this function does the work in most cases. :is_flush: whether to flush data to file for each row written to file Parameters: Name Type Description Default data_inp dict or list of dict to be saved required output path to output file required Returns: Type Description None Source code in haferml/sync/local.py def save_records ( data_inp , output , is_flush = None , write_mode = None ): \"\"\"Save list of dicts to file. Instead of loading pandas for such a simple job, this function does the work in most cases. :param data_inp: dict or list of dict to be saved :param output: path to output file :is_flush: whether to flush data to file for each row written to file :return: None \"\"\" if write_mode is None : write_mode = \"a+\" if is_flush is None : is_flush = False if isinstance ( data_inp , list ): data = data_inp elif isinstance ( data_inp , dict ): data = [ data_inp ] else : raise Exception ( \"Input data is neither list nor dict: {} \" . format ( data_inp )) try : with open ( output , write_mode ) as fp : for i in data : json . dump ( i , fp ) fp . write ( \" \\n \" ) if is_flush : fp . flush () except Exception as ee : raise Exception ( \"Could not load data to file: {} \" . format ( ee ))","title":"save_records()"},{"location":"references/utils/","text":"Utils \u00a4 haferml.utils module contains the utilities.","title":"Index"},{"location":"references/utils/#utils","text":"haferml.utils module contains the utilities.","title":"Utils"},{"location":"references/utils/sugar/","text":"Utils - Sugar \u00a4 check_env ( var_list ) \u00a4 check_env checks if the given environment variables exists Parameters: Name Type Description Default var_list list of variables to be checked required Source code in haferml/utils/sugar.py def check_env ( var_list ): \"\"\" check_env checks if the given environment variables exists :param var_list: list of variables to be checked :type definition: list \"\"\" missing = [] res = {} for envvar in var_list : envvar_val = os . getenv ( envvar ) if envvar_val is None : missing . append ( envvar ) res [ envvar ] = envvar_val if missing : raise Exception ( f \"Missing envs { missing } \" ) return res","title":"utils.sugar"},{"location":"references/utils/sugar/#utils-sugar","text":"","title":"Utils - Sugar"},{"location":"references/utils/sugar/#haferml.utils.sugar.check_env","text":"check_env checks if the given environment variables exists Parameters: Name Type Description Default var_list list of variables to be checked required Source code in haferml/utils/sugar.py def check_env ( var_list ): \"\"\" check_env checks if the given environment variables exists :param var_list: list of variables to be checked :type definition: list \"\"\" missing = [] res = {} for envvar in var_list : envvar_val = os . getenv ( envvar ) if envvar_val is None : missing . append ( envvar ) res [ envvar ] = envvar_val if missing : raise Exception ( f \"Missing envs { missing } \" ) return res","title":"check_env()"},{"location":"tutorials/","text":"Tutorials \u00a4 Some tutorials are provided here in this section. For the code being used in this tutorial, please checkout the repo emptymalei/haferml-tutorials on GitHub .","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"Some tutorials are provided here in this section. For the code being used in this tutorial, please checkout the repo emptymalei/haferml-tutorials on GitHub .","title":"Tutorials"},{"location":"tutorials/build_models/","text":"Build Models \u00a4 HaferML provides three main classes for model building, haferml.model.dataset.DataSet , which holds the data and necessary functions such as creating train test splits, haferml.model.modelset.ModelSet , which holds the model definition and supporting functions such as hyperparameter related functions, haferml.model.workflow.ModelWorkflow , which is used to combine the DataSet and ModelSet. The reason that we split the DataSet and ModelSet is that we will mostly need ModelSet in predictions. Thus when using the model, we will only load the ModelSet.","title":"Introduction"},{"location":"tutorials/build_models/#build-models","text":"HaferML provides three main classes for model building, haferml.model.dataset.DataSet , which holds the data and necessary functions such as creating train test splits, haferml.model.modelset.ModelSet , which holds the model definition and supporting functions such as hyperparameter related functions, haferml.model.workflow.ModelWorkflow , which is used to combine the DataSet and ModelSet. The reason that we split the DataSet and ModelSet is that we will mostly need ModelSet in predictions. Thus when using the model, we will only load the ModelSet.","title":"Build Models"},{"location":"tutorials/data_manipulation/","text":"Data Manipulation \u00a4 There are several philosophies in transforming the dataset. For example, some methods will transform the data in a column based style while some other methods will perform the transformation in a row based style. HaferML implemented utilities for both of these styles. Transformer \u00a4 haferml.etl.transform.pipeline.Transformer is a row-based transformer that transforms one row into the data we need. There are several advantages of this type of transformation, we can simply drop the record if it can not be used, and we can easily stream the data as it comes in. Ordered Transformer \u00a4 HaferML does not have a dedicated transformer for column-based transformations. The reason is that we can easily do this using haferml.preprocess.ingredients.OrderedProcessor or build our own using haferml.preprocess.ingredients.with_transforms and haferml.preprocess.ingredients.order or haferml.preprocess.ingredients.attributes with each step being some column based operations. With pandas, column based transformations are much easier than the previous row-based method.","title":"Introduction"},{"location":"tutorials/data_manipulation/#data-manipulation","text":"There are several philosophies in transforming the dataset. For example, some methods will transform the data in a column based style while some other methods will perform the transformation in a row based style. HaferML implemented utilities for both of these styles.","title":"Data Manipulation"},{"location":"tutorials/data_manipulation/#transformer","text":"haferml.etl.transform.pipeline.Transformer is a row-based transformer that transforms one row into the data we need. There are several advantages of this type of transformation, we can simply drop the record if it can not be used, and we can easily stream the data as it comes in.","title":"Transformer"},{"location":"tutorials/data_manipulation/#ordered-transformer","text":"HaferML does not have a dedicated transformer for column-based transformations. The reason is that we can easily do this using haferml.preprocess.ingredients.OrderedProcessor or build our own using haferml.preprocess.ingredients.with_transforms and haferml.preprocess.ingredients.order or haferml.preprocess.ingredients.attributes with each step being some column based operations. With pandas, column based transformations are much easier than the previous row-based method.","title":"Ordered Transformer"},{"location":"tutorials/preservation/","text":"Preservation \u00a4 HaferML provides functions and utilities to sync data and model artifacts with cloud storage, e.g. S3. Warning We only support S3 at this moment. However, this can be easily extended to other platforms. Upload to S3 \u00a4 See haferml.sync.aws.aws_cli for examples.","title":"Introduction"},{"location":"tutorials/preservation/#preservation","text":"HaferML provides functions and utilities to sync data and model artifacts with cloud storage, e.g. S3. Warning We only support S3 at this moment. However, this can be easily extended to other platforms.","title":"Preservation"},{"location":"tutorials/preservation/#upload-to-s3","text":"See haferml.sync.aws.aws_cli for examples.","title":"Upload to S3"},{"location":"tutorials/rideindego/","text":"Rideindego Data \u00a4 This is a tutorial for rideindego data. The code for this project is on GitHub . Everything in the artifacts folder can be synced to AWS S3 using haferml.sync.aws function or manually upload using aws cli. The whole project will then be restored by redownload the artifacts.","title":"Introduction"},{"location":"tutorials/rideindego/#rideindego-data","text":"This is a tutorial for rideindego data. The code for this project is on GitHub . Everything in the artifacts folder can be synced to AWS S3 using haferml.sync.aws function or manually upload using aws cli. The whole project will then be restored by redownload the artifacts.","title":"Rideindego Data"},{"location":"tutorials/rideindego/config/","text":"Rideindego Config \u00a4 We use one single global config for the whole project. It is highly recommended to place the config json file inside the artifacts folder so that we can restore everything later when syncing the whole artifacts folder. { \"name\" : \"rideindego prices\" , \"etl\" : { \"cache_folder\" : \"cache\" , \"raw\" : { \"local\" : \"data/raw\" , \"remote\" : \"s3://haferml-rideindego/marshall/data/raw\" , \"stations\" : { \"name\" : \"stations.parquet\" , \"local\" : \"data/raw/stations\" , \"remote\" : \"s3://haferml-rideindego/marshall/data/raw/stations\" }, \"trip_data\" : { \"source\" : \"https://www.rideindego.com/about/data/\" , \"name\" : \"trips.parquet\" , \"local\" : \"data/raw/trip_data\" , \"remote\" : \"s3://haferml-rideindego/marshall/data/raw/trip_data\" } }, \"transformed\" : { \"local\" : \"dataset/etl\" , \"remote\" : \"s3://haferml-rideindego/marshall/dataset/etl\" , \"stations\" : { \"name\" : \"stations.parquet\" , \"local\" : \"dataset/etl\" , \"remote\" : \"s3://haferml-rideindego/marshall/dataset/etl\" }, \"trip_data\" : { \"name\" : \"trips.parquet\" , \"local\" : \"dataset/etl\" , \"remote\" : \"s3://haferml-rideindego/marshall/dataset/etl\" } } }, \"preprocessing\" : { \"dataset\" : { \"local\" : \"model/dataset\" , \"remote\" : \"s3://haferml-rideindego/marshall/model/dataset\" , \"preprocessed\" : { \"name\" : \"preprocessed.parquet\" , \"local\" : \"model/dataset\" , \"remote\" : \"s3://haferml-rideindego/marshall/model/dataset\" } }, \"features\" : [ \"passholder_type\" , \"bike_type\" , \"trip_route_category\" , \"hour\" , \"weekday\" , \"month\" ], \"targets\" : [ \"duration\" ], \"feature_engineering\" : { }, \"target_engineering\" : { } }, \"model\" : { \"rf\" : { \"features\" : [ \"passholder_type\" , \"bike_type\" , \"trip_route_category\" , \"hour\" , \"weekday\" , \"month\" ], \"targets\" : [ \"duration\" ], \"encoding\" : { \"categorical_columns\" : [ \"passholder_type\" , \"bike_type\" , \"trip_route_category\" ] }, \"random_state\" : 42 , \"test_size\" : 0.3 , \"cv\" : { \"folds\" : 3 , \"verbose\" : 6 , \"n_jobs\" : 1 , \"n_iter\" : 5 }, \"hyperparameters\" : {}, \"artifacts\" : { \"dataset\" : { \"local\" : \"model/dataset\" , \"remote\" : \"s3://haferml-rideindego/marshall/model/dataset\" }, \"model\" : { \"name\" : \"model.joblib\" , \"local\" : \"model/model\" , \"remote\" : \"s3://haferml-rideindego/marshall/model/model\" }, \"prediction\" : { \"local\" : \"prediction\" , \"remote\" : \"s3://haferml-rideindego/marshall/prediction\" }, \"performance\" : { \"local\" : \"performance\" , \"remote\" : \"s3://haferml-rideindego/marshall/performance\" } } } } }","title":"Config"},{"location":"tutorials/rideindego/config/#rideindego-config","text":"We use one single global config for the whole project. It is highly recommended to place the config json file inside the artifacts folder so that we can restore everything later when syncing the whole artifacts folder. { \"name\" : \"rideindego prices\" , \"etl\" : { \"cache_folder\" : \"cache\" , \"raw\" : { \"local\" : \"data/raw\" , \"remote\" : \"s3://haferml-rideindego/marshall/data/raw\" , \"stations\" : { \"name\" : \"stations.parquet\" , \"local\" : \"data/raw/stations\" , \"remote\" : \"s3://haferml-rideindego/marshall/data/raw/stations\" }, \"trip_data\" : { \"source\" : \"https://www.rideindego.com/about/data/\" , \"name\" : \"trips.parquet\" , \"local\" : \"data/raw/trip_data\" , \"remote\" : \"s3://haferml-rideindego/marshall/data/raw/trip_data\" } }, \"transformed\" : { \"local\" : \"dataset/etl\" , \"remote\" : \"s3://haferml-rideindego/marshall/dataset/etl\" , \"stations\" : { \"name\" : \"stations.parquet\" , \"local\" : \"dataset/etl\" , \"remote\" : \"s3://haferml-rideindego/marshall/dataset/etl\" }, \"trip_data\" : { \"name\" : \"trips.parquet\" , \"local\" : \"dataset/etl\" , \"remote\" : \"s3://haferml-rideindego/marshall/dataset/etl\" } } }, \"preprocessing\" : { \"dataset\" : { \"local\" : \"model/dataset\" , \"remote\" : \"s3://haferml-rideindego/marshall/model/dataset\" , \"preprocessed\" : { \"name\" : \"preprocessed.parquet\" , \"local\" : \"model/dataset\" , \"remote\" : \"s3://haferml-rideindego/marshall/model/dataset\" } }, \"features\" : [ \"passholder_type\" , \"bike_type\" , \"trip_route_category\" , \"hour\" , \"weekday\" , \"month\" ], \"targets\" : [ \"duration\" ], \"feature_engineering\" : { }, \"target_engineering\" : { } }, \"model\" : { \"rf\" : { \"features\" : [ \"passholder_type\" , \"bike_type\" , \"trip_route_category\" , \"hour\" , \"weekday\" , \"month\" ], \"targets\" : [ \"duration\" ], \"encoding\" : { \"categorical_columns\" : [ \"passholder_type\" , \"bike_type\" , \"trip_route_category\" ] }, \"random_state\" : 42 , \"test_size\" : 0.3 , \"cv\" : { \"folds\" : 3 , \"verbose\" : 6 , \"n_jobs\" : 1 , \"n_iter\" : 5 }, \"hyperparameters\" : {}, \"artifacts\" : { \"dataset\" : { \"local\" : \"model/dataset\" , \"remote\" : \"s3://haferml-rideindego/marshall/model/dataset\" }, \"model\" : { \"name\" : \"model.joblib\" , \"local\" : \"model/model\" , \"remote\" : \"s3://haferml-rideindego/marshall/model/model\" }, \"prediction\" : { \"local\" : \"prediction\" , \"remote\" : \"s3://haferml-rideindego/marshall/prediction\" }, \"performance\" : { \"local\" : \"performance\" , \"remote\" : \"s3://haferml-rideindego/marshall/performance\" } } } } }","title":"Rideindego Config"},{"location":"tutorials/rideindego/extract/","text":"Extract \u00a4 As a first step, we will download the all the data files from the official website of Rideindego . The web page contains multiple links to the file. We will get all the links to the data files using DataLinkHTMLExtractor defined in utils.fetch . We will then run through each link and download the zip file using DataDownloader . import os import click import simplejson as json from dotenv import load_dotenv from loguru import logger from utils.fetch import DataDownloader , DataLinkHTMLExtractor from utils.fetch import get_page_html as _get_page_html from haferml.blend.config import Config from haferml.sync.local import prepare_folders load_dotenv () @click . command () @click . option ( \"-c\" , \"--config\" , type = str , default = os . getenv ( \"CONFIG_FILE\" ), help = \"Path to config file\" , ) def extract ( config ): base_folder = os . getenv ( \"BASE_FOLDER\" ) _CONFIG = Config ( config , base_folder = base_folder ) etl_trip_data_config = _CONFIG . get ([ \"etl\" , \"raw\" , \"trip_data\" ]) logger . info ( f \"Using config: { etl_trip_data_config } \" ) # create folders prepare_folders ( etl_trip_data_config [ \"local\" ], base_folder ) # if not os.path.exists(etl_trip_data_config[\"local\"]): # os.makedirs(etl_trip_data_config[\"local\"]) # Download Raw Data source_link = etl_trip_data_config [ \"source\" ] logger . info ( f \"Will download from { source_link } \" ) page = _get_page_html ( source_link ) . get ( \"data\" , {}) page_extractor = DataLinkHTMLExtractor ( page ) links = page_extractor . get_data_links () logger . info ( f \"Extracted links from { source_link } : { links } \" ) # Download data dld = DataDownloader ( links , data_type = \"zip\" , folder = etl_trip_data_config [ \"local_absolute\" ]) dld . run () if __name__ == \"__main__\" : extract ()","title":"Extract"},{"location":"tutorials/rideindego/extract/#extract","text":"As a first step, we will download the all the data files from the official website of Rideindego . The web page contains multiple links to the file. We will get all the links to the data files using DataLinkHTMLExtractor defined in utils.fetch . We will then run through each link and download the zip file using DataDownloader . import os import click import simplejson as json from dotenv import load_dotenv from loguru import logger from utils.fetch import DataDownloader , DataLinkHTMLExtractor from utils.fetch import get_page_html as _get_page_html from haferml.blend.config import Config from haferml.sync.local import prepare_folders load_dotenv () @click . command () @click . option ( \"-c\" , \"--config\" , type = str , default = os . getenv ( \"CONFIG_FILE\" ), help = \"Path to config file\" , ) def extract ( config ): base_folder = os . getenv ( \"BASE_FOLDER\" ) _CONFIG = Config ( config , base_folder = base_folder ) etl_trip_data_config = _CONFIG . get ([ \"etl\" , \"raw\" , \"trip_data\" ]) logger . info ( f \"Using config: { etl_trip_data_config } \" ) # create folders prepare_folders ( etl_trip_data_config [ \"local\" ], base_folder ) # if not os.path.exists(etl_trip_data_config[\"local\"]): # os.makedirs(etl_trip_data_config[\"local\"]) # Download Raw Data source_link = etl_trip_data_config [ \"source\" ] logger . info ( f \"Will download from { source_link } \" ) page = _get_page_html ( source_link ) . get ( \"data\" , {}) page_extractor = DataLinkHTMLExtractor ( page ) links = page_extractor . get_data_links () logger . info ( f \"Extracted links from { source_link } : { links } \" ) # Download data dld = DataDownloader ( links , data_type = \"zip\" , folder = etl_trip_data_config [ \"local_absolute\" ]) dld . run () if __name__ == \"__main__\" : extract ()","title":"Extract"},{"location":"tutorials/rideindego/prediction/","text":"Prediction \u00a4 To use the model, we will reload the model from the artifacts saved in the training step. It's very easy to reload the model, e.g., ModelSet in the following example. import os import click import joblib import pandas as pd from dotenv import load_dotenv from haferml.blend.config import Config from haferml.model.pipeline import ModelSetX from haferml.sync.local import prepare_folders from loguru import logger load_dotenv () class ModelSet ( ModelSetX ): \"\"\" The core of the model including hyperparameters \"\"\" def __init__ ( self , config , base_folder ): super ( ModelSet , self ) . __init__ ( config , base_folder ) def reload ( self ): model_folder = self . artifacts [ \"model\" ][ \"local_absolute\" ] logger . info ( \"Reload models\" ) self . model = joblib . load ( os . path . join ( self . base_folder , model_folder , self . artifacts [ \"model\" ][ \"name\" ], ) ) def predict ( self , data ): return self . model . predict ( data ) @click . command () @click . option ( \"-c\" , \"--config\" , type = str , default = os . getenv ( \"CONFIG_FILE\" ), help = \"Path to config file\" , ) def predict ( config ): base_folder = os . getenv ( \"BASE_FOLDER\" ) logger . debug ( f \"base folder is: { base_folder } \" ) logger . debug ( f \"config: { config } \" ) _CONFIG = Config ( config , base_folder = base_folder ) preprocessed_data_config = _CONFIG [[ \"preprocessing\" , \"dataset\" , \"preprocessed\" ]] rf_config = _CONFIG [[ \"model\" , \"rf\" ]] # create folders prepare_folders ( _CONFIG [[ \"model\" , \"rf\" , \"artifacts\" , \"model\" , \"local\" ]], base_folder = base_folder ) # load some data dataset_folder = _CONFIG [[ \"model\" , \"rf\" , \"artifacts\" , \"dataset\" , \"local_absolute\" ]] df = pd . read_parquet ( os . path . join ( dataset_folder , \"model_X_test.parquet\" ) ) . sample ( 1 ) # model logger . debug ( f \"Prepare modelset and dataset\" ) M = ModelSet ( config = rf_config , base_folder = base_folder ) M . reload () logger . info ( M . predict ( df ) ) if __name__ == \"__main__\" : predict ()","title":"Prediction"},{"location":"tutorials/rideindego/prediction/#prediction","text":"To use the model, we will reload the model from the artifacts saved in the training step. It's very easy to reload the model, e.g., ModelSet in the following example. import os import click import joblib import pandas as pd from dotenv import load_dotenv from haferml.blend.config import Config from haferml.model.pipeline import ModelSetX from haferml.sync.local import prepare_folders from loguru import logger load_dotenv () class ModelSet ( ModelSetX ): \"\"\" The core of the model including hyperparameters \"\"\" def __init__ ( self , config , base_folder ): super ( ModelSet , self ) . __init__ ( config , base_folder ) def reload ( self ): model_folder = self . artifacts [ \"model\" ][ \"local_absolute\" ] logger . info ( \"Reload models\" ) self . model = joblib . load ( os . path . join ( self . base_folder , model_folder , self . artifacts [ \"model\" ][ \"name\" ], ) ) def predict ( self , data ): return self . model . predict ( data ) @click . command () @click . option ( \"-c\" , \"--config\" , type = str , default = os . getenv ( \"CONFIG_FILE\" ), help = \"Path to config file\" , ) def predict ( config ): base_folder = os . getenv ( \"BASE_FOLDER\" ) logger . debug ( f \"base folder is: { base_folder } \" ) logger . debug ( f \"config: { config } \" ) _CONFIG = Config ( config , base_folder = base_folder ) preprocessed_data_config = _CONFIG [[ \"preprocessing\" , \"dataset\" , \"preprocessed\" ]] rf_config = _CONFIG [[ \"model\" , \"rf\" ]] # create folders prepare_folders ( _CONFIG [[ \"model\" , \"rf\" , \"artifacts\" , \"model\" , \"local\" ]], base_folder = base_folder ) # load some data dataset_folder = _CONFIG [[ \"model\" , \"rf\" , \"artifacts\" , \"dataset\" , \"local_absolute\" ]] df = pd . read_parquet ( os . path . join ( dataset_folder , \"model_X_test.parquet\" ) ) . sample ( 1 ) # model logger . debug ( f \"Prepare modelset and dataset\" ) M = ModelSet ( config = rf_config , base_folder = base_folder ) M . reload () logger . info ( M . predict ( df ) ) if __name__ == \"__main__\" : predict ()","title":"Prediction"},{"location":"tutorials/rideindego/preprocessing/","text":"Preprocessing \u00a4 Preprocessing is using ordered member function by inheriting from BasePreProcessor . import datetime import os import click import pandas as pd import simplejson as json from dotenv import load_dotenv from haferml.blend.config import Config from haferml.preprocess.ingredients import attributes from haferml.preprocess.pipeline import BasePreProcessor from haferml.sync.local import prepare_folders from loguru import logger logger . info ( f \"Experiment started at: { datetime . datetime . now () } \" ) load_dotenv () def load_data ( data_path ): if data_path . endswith ( \".parquet\" ): dataframe = pd . read_parquet ( data_path ) else : raise ValueError ( f \"Input path file format is not supported: { data_path } \" ) return dataframe class Preprocess ( BasePreProcessor ): \"\"\" Preprocess dataset There is very little to preprocess in this example. But we will keep this class for illustration purpose. \"\"\" def __init__ ( self , config ): super ( Preprocess , self ) . __init__ ( config = config ) self . feature_cols = self . config [ \"features\" ] self . target_cols = self . config [ \"targets\" ] @attributes ( order = 1 ) def _drop_unused_columns ( self , dataframe ): self . dataframe = dataframe [ self . feature_cols + self . target_cols ] return self . dataframe @click . command () @click . option ( \"-c\" , \"--config\" , type = str , default = os . getenv ( \"CONFIG_FILE\" ), help = \"Path to config file\" , ) def preprocess ( config ): base_folder = os . getenv ( \"BASE_FOLDER\" ) _CONFIG = Config ( config , base_folder = base_folder ) preprocessed_data_config = _CONFIG [[ \"preprocessing\" , \"dataset\" , \"preprocessed\" ]] transformed_trip_data_config = _CONFIG [[ \"etl\" , \"transformed\" , \"trip_data\" ]] # create folders prepare_folders ( preprocessed_data_config [ \"local\" ], base_folder = base_folder ) prepare_folders ( transformed_trip_data_config [ \"local\" ], base_folder = base_folder ) # load transformed data df = load_data ( transformed_trip_data_config [ \"name_absolute\" ]) # preprocess pr = Preprocess ( config = _CONFIG [[ \"preprocessing\" ]]) df = pr . run ( df ) # save df . to_parquet ( preprocessed_data_config [ \"name_absolute\" ], index = False ) logger . info ( f 'Saved preprocessed data to { preprocessed_data_config [ \"name_absolute\" ] } ' ) return df if __name__ == \"__main__\" : preprocess ()","title":"Preprocessing"},{"location":"tutorials/rideindego/preprocessing/#preprocessing","text":"Preprocessing is using ordered member function by inheriting from BasePreProcessor . import datetime import os import click import pandas as pd import simplejson as json from dotenv import load_dotenv from haferml.blend.config import Config from haferml.preprocess.ingredients import attributes from haferml.preprocess.pipeline import BasePreProcessor from haferml.sync.local import prepare_folders from loguru import logger logger . info ( f \"Experiment started at: { datetime . datetime . now () } \" ) load_dotenv () def load_data ( data_path ): if data_path . endswith ( \".parquet\" ): dataframe = pd . read_parquet ( data_path ) else : raise ValueError ( f \"Input path file format is not supported: { data_path } \" ) return dataframe class Preprocess ( BasePreProcessor ): \"\"\" Preprocess dataset There is very little to preprocess in this example. But we will keep this class for illustration purpose. \"\"\" def __init__ ( self , config ): super ( Preprocess , self ) . __init__ ( config = config ) self . feature_cols = self . config [ \"features\" ] self . target_cols = self . config [ \"targets\" ] @attributes ( order = 1 ) def _drop_unused_columns ( self , dataframe ): self . dataframe = dataframe [ self . feature_cols + self . target_cols ] return self . dataframe @click . command () @click . option ( \"-c\" , \"--config\" , type = str , default = os . getenv ( \"CONFIG_FILE\" ), help = \"Path to config file\" , ) def preprocess ( config ): base_folder = os . getenv ( \"BASE_FOLDER\" ) _CONFIG = Config ( config , base_folder = base_folder ) preprocessed_data_config = _CONFIG [[ \"preprocessing\" , \"dataset\" , \"preprocessed\" ]] transformed_trip_data_config = _CONFIG [[ \"etl\" , \"transformed\" , \"trip_data\" ]] # create folders prepare_folders ( preprocessed_data_config [ \"local\" ], base_folder = base_folder ) prepare_folders ( transformed_trip_data_config [ \"local\" ], base_folder = base_folder ) # load transformed data df = load_data ( transformed_trip_data_config [ \"name_absolute\" ]) # preprocess pr = Preprocess ( config = _CONFIG [[ \"preprocessing\" ]]) df = pr . run ( df ) # save df . to_parquet ( preprocessed_data_config [ \"name_absolute\" ], index = False ) logger . info ( f 'Saved preprocessed data to { preprocessed_data_config [ \"name_absolute\" ] } ' ) return df if __name__ == \"__main__\" : preprocess ()","title":"Preprocessing"},{"location":"tutorials/rideindego/random-forest/","text":"Random Forest \u00a4 As an example, we will build a simple random forest model. This is only a demonstration of the package so we do not care about the performance. import datetime import os import category_encoders as ce import click import joblib import numpy as np import pandas as pd from dotenv import load_dotenv from haferml.blend.config import Config from haferml.model.pipeline import DataSetX , ModelSetX , ModelWorkflowX from haferml.sync.local import prepare_folders from haferml.sync.local import isoencode from loguru import logger from sklearn import metrics from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import RandomizedSearchCV , train_test_split from sklearn.pipeline import Pipeline logger . info ( f \"Experiment started at: { datetime . datetime . now () } \" ) load_dotenv () def load_data ( data_path ): if data_path . endswith ( \".parquet\" ): dataframe = pd . read_parquet ( data_path ) else : raise ValueError ( f \"Input path file format is not supported: { data_path } \" ) return dataframe class DataSet ( DataSetX ): \"\"\" DataSet for the model \"\"\" def __init__ ( self , config , base_folder ): super ( DataSet , self ) . __init__ ( config , base_folder ) self . targets = self . config . get ( \"targets\" ) self . features = self . config . get ( \"features\" ) self . cat_cols = self . config . get ( \"encoding\" , {}) . get ( \"categorical_columns\" ) self . test_size = self . config . get ( \"test_size\" ) self . random_state = self . config . get ( \"random_state\" ) logger . debug ( f \"features: { self . features } \\n \" f \"predict: { self . targets } \\n \" f \"base folder: { self . base_folder } \\n \" f \"artifacts configs: { self . artifacts } \" ) def _encode ( self ): self . cat_encoder = ce . BinaryEncoder ( cols = self . cat_cols ) self . cat_encoder . fit ( self . X , self . y ) def create_train_test_datasets ( self , data ): self . data = data logger . debug ( f \"length of dataset: { len ( self . data ) } \\n \" ) self . X = self . data . loc [:, self . features ] self . y = self . data . loc [:, self . targets ] # No information leak here as we use Binary Encoder only self . _encode () self . X = self . cat_encoder . transform ( self . X , self . y ) logger . debug ( \"Splitting dataset\" ) self . X_train , self . X_test , self . y_train , self . y_test = train_test_split ( self . X , self . y , test_size = self . test_size , random_state = self . random_state ) logger . debug ( \"Shape of train and test data: \\n \" f \"X_train: { self . X_train . shape } \\n \" f \"y_train: { self . y_train . shape } \\n \" f \"X_test: { self . X_test . shape } \\n \" f \"y_test: { self . y_test . shape } \\n \" ) # save the train test data self . _export_train_test_data () class ModelSet ( ModelSetX ): \"\"\" The core of the model including hyperparameters \"\"\" def __init__ ( self , config , base_folder ): super ( ModelSet , self ) . __init__ ( config , base_folder ) self . test_size = self . config . get ( \"test_size\" ) self . random_state = self . config . get ( \"random_state\" ) logger . debug ( f \"features: { self . features } \\n \" f \"predict: { self . targets } \\n \" f \"base folder: { self . base_folder } \\n \" f \"artifacts configs: { self . artifacts } \" ) def create_model ( self ): logger . info ( \"Setting up hyperparameters ...\" ) logger . info ( \"Create pipeline\" ) rf = RandomForestRegressor ( random_state = self . random_state , oob_score = False , n_jobs =- 1 ) self . pipeline_steps = [( \"model\" , rf )] self . pipeline = Pipeline ( self . pipeline_steps ) logger . info ( \"Create model with CV\" ) self . model = RandomizedSearchCV ( self . pipeline , cv = self . config . get ( \"cv\" , {}) . get ( \"folds\" , 3 ), n_iter = self . config . get ( \"cv\" , {}) . get ( \"n_iter\" , 5 ), param_distributions = self . hyperparameters , verbose = 6 , ) def _set_hyperparameters ( self ): hyperparams_grid = self . config . get ( \"hyperparameters\" ) if hyperparams_grid is None : hyperparams_grid = self . _create_hyperparameter_space () else : hyperparams_grid = { ** ( self . _create_hyperparameter_space ()), ** hyperparams_grid , } logger . info ( f \"Using hyperparameters: \\n { hyperparams_grid } \" ) return hyperparams_grid @staticmethod def _create_hyperparameter_space (): \"\"\" _create_hyperparameter_space creates a set of hyperparameters for the random forest \"\"\" # Number of trees in random forest # n_estimators = [int(x) for x in np.linspace(50, 150, 5)] n_estimators = [ 90 , 100 , 110 , 120 ] # Number of features to consider at every split max_features = [ \"auto\" , 0.9 , 0.8 ] # Maximum number of levels in tree # max_depth = [int(x) for x in range(10, 20, 2)] max_depth = [ None ] # max_depth.append(None) # Minimum number of samples required to split a node # min_samples_split = [2, 4, 6] min_samples_split = [ 2 ] # Minimum number of samples required at each leaf node # min_samples_leaf = [1, 2, 3] min_samples_leaf = [ 1 ] # Method of selecting samples for training each tree bootstrap = [ True ] # feature_selection__k = [15, 20, 25, 30, 35, 40, 45, 50] rf_random_grid = { \"model__n_estimators\" : n_estimators , \"model__max_features\" : max_features , \"model__max_depth\" : max_depth , \"model__min_samples_split\" : min_samples_split , \"model__min_samples_leaf\" : min_samples_leaf , \"model__bootstrap\" : bootstrap , } return rf_random_grid class RandomForest ( ModelWorkflowX ): \"\"\"A model to predict the duration\"\"\" def __init__ ( self , config , dataset , modelset , base_folder = os . getenv ( \"BASE_FOLDER\" )): super ( RandomForest , self ) . __init__ ( config , dataset , modelset , base_folder ) self . name = \"marshall random forest\" @click . command () @click . option ( \"-c\" , \"--config\" , type = str , default = os . getenv ( \"CONFIG_FILE\" ), help = \"Path to config file\" , ) @click . option ( \"--test/--no-test\" , type = bool , default = False , help = \"Flag for test\" , ) def preprocess ( config , test ): base_folder = os . getenv ( \"BASE_FOLDER\" ) _CONFIG = Config ( config , base_folder = base_folder ) preprocessed_data_config = _CONFIG [[ \"preprocessing\" , \"dataset\" , \"preprocessed\" ]] rf_config = _CONFIG [[ \"model\" , \"rf\" ]] # create folders prepare_folders ( preprocessed_data_config [ \"local\" ], base_folder = base_folder ) prepare_folders ( _CONFIG [[ \"model\" , \"rf\" , \"artifacts\" , \"model\" , \"local\" ]], base_folder = base_folder ) # load transformed data logger . debug ( f \"Loading data ...\" ) df = load_data ( preprocessed_data_config [ \"name_absolute\" ]) if test is True : df = df . sample ( 1000 ) # model logger . debug ( f \"Prepare modelset and dataset\" ) D = DataSet ( config = rf_config , base_folder = base_folder ) M = ModelSet ( config = rf_config , base_folder = base_folder ) logger . debug ( f \"Assemble randomforest\" ) rf = RandomForest ( config = rf_config , dataset = D , modelset = M ) logger . debug ( f \"Training ...\" ) rf . train ( df ) if __name__ == \"__main__\" : preprocess ()","title":"Random Forest"},{"location":"tutorials/rideindego/random-forest/#random-forest","text":"As an example, we will build a simple random forest model. This is only a demonstration of the package so we do not care about the performance. import datetime import os import category_encoders as ce import click import joblib import numpy as np import pandas as pd from dotenv import load_dotenv from haferml.blend.config import Config from haferml.model.pipeline import DataSetX , ModelSetX , ModelWorkflowX from haferml.sync.local import prepare_folders from haferml.sync.local import isoencode from loguru import logger from sklearn import metrics from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import RandomizedSearchCV , train_test_split from sklearn.pipeline import Pipeline logger . info ( f \"Experiment started at: { datetime . datetime . now () } \" ) load_dotenv () def load_data ( data_path ): if data_path . endswith ( \".parquet\" ): dataframe = pd . read_parquet ( data_path ) else : raise ValueError ( f \"Input path file format is not supported: { data_path } \" ) return dataframe class DataSet ( DataSetX ): \"\"\" DataSet for the model \"\"\" def __init__ ( self , config , base_folder ): super ( DataSet , self ) . __init__ ( config , base_folder ) self . targets = self . config . get ( \"targets\" ) self . features = self . config . get ( \"features\" ) self . cat_cols = self . config . get ( \"encoding\" , {}) . get ( \"categorical_columns\" ) self . test_size = self . config . get ( \"test_size\" ) self . random_state = self . config . get ( \"random_state\" ) logger . debug ( f \"features: { self . features } \\n \" f \"predict: { self . targets } \\n \" f \"base folder: { self . base_folder } \\n \" f \"artifacts configs: { self . artifacts } \" ) def _encode ( self ): self . cat_encoder = ce . BinaryEncoder ( cols = self . cat_cols ) self . cat_encoder . fit ( self . X , self . y ) def create_train_test_datasets ( self , data ): self . data = data logger . debug ( f \"length of dataset: { len ( self . data ) } \\n \" ) self . X = self . data . loc [:, self . features ] self . y = self . data . loc [:, self . targets ] # No information leak here as we use Binary Encoder only self . _encode () self . X = self . cat_encoder . transform ( self . X , self . y ) logger . debug ( \"Splitting dataset\" ) self . X_train , self . X_test , self . y_train , self . y_test = train_test_split ( self . X , self . y , test_size = self . test_size , random_state = self . random_state ) logger . debug ( \"Shape of train and test data: \\n \" f \"X_train: { self . X_train . shape } \\n \" f \"y_train: { self . y_train . shape } \\n \" f \"X_test: { self . X_test . shape } \\n \" f \"y_test: { self . y_test . shape } \\n \" ) # save the train test data self . _export_train_test_data () class ModelSet ( ModelSetX ): \"\"\" The core of the model including hyperparameters \"\"\" def __init__ ( self , config , base_folder ): super ( ModelSet , self ) . __init__ ( config , base_folder ) self . test_size = self . config . get ( \"test_size\" ) self . random_state = self . config . get ( \"random_state\" ) logger . debug ( f \"features: { self . features } \\n \" f \"predict: { self . targets } \\n \" f \"base folder: { self . base_folder } \\n \" f \"artifacts configs: { self . artifacts } \" ) def create_model ( self ): logger . info ( \"Setting up hyperparameters ...\" ) logger . info ( \"Create pipeline\" ) rf = RandomForestRegressor ( random_state = self . random_state , oob_score = False , n_jobs =- 1 ) self . pipeline_steps = [( \"model\" , rf )] self . pipeline = Pipeline ( self . pipeline_steps ) logger . info ( \"Create model with CV\" ) self . model = RandomizedSearchCV ( self . pipeline , cv = self . config . get ( \"cv\" , {}) . get ( \"folds\" , 3 ), n_iter = self . config . get ( \"cv\" , {}) . get ( \"n_iter\" , 5 ), param_distributions = self . hyperparameters , verbose = 6 , ) def _set_hyperparameters ( self ): hyperparams_grid = self . config . get ( \"hyperparameters\" ) if hyperparams_grid is None : hyperparams_grid = self . _create_hyperparameter_space () else : hyperparams_grid = { ** ( self . _create_hyperparameter_space ()), ** hyperparams_grid , } logger . info ( f \"Using hyperparameters: \\n { hyperparams_grid } \" ) return hyperparams_grid @staticmethod def _create_hyperparameter_space (): \"\"\" _create_hyperparameter_space creates a set of hyperparameters for the random forest \"\"\" # Number of trees in random forest # n_estimators = [int(x) for x in np.linspace(50, 150, 5)] n_estimators = [ 90 , 100 , 110 , 120 ] # Number of features to consider at every split max_features = [ \"auto\" , 0.9 , 0.8 ] # Maximum number of levels in tree # max_depth = [int(x) for x in range(10, 20, 2)] max_depth = [ None ] # max_depth.append(None) # Minimum number of samples required to split a node # min_samples_split = [2, 4, 6] min_samples_split = [ 2 ] # Minimum number of samples required at each leaf node # min_samples_leaf = [1, 2, 3] min_samples_leaf = [ 1 ] # Method of selecting samples for training each tree bootstrap = [ True ] # feature_selection__k = [15, 20, 25, 30, 35, 40, 45, 50] rf_random_grid = { \"model__n_estimators\" : n_estimators , \"model__max_features\" : max_features , \"model__max_depth\" : max_depth , \"model__min_samples_split\" : min_samples_split , \"model__min_samples_leaf\" : min_samples_leaf , \"model__bootstrap\" : bootstrap , } return rf_random_grid class RandomForest ( ModelWorkflowX ): \"\"\"A model to predict the duration\"\"\" def __init__ ( self , config , dataset , modelset , base_folder = os . getenv ( \"BASE_FOLDER\" )): super ( RandomForest , self ) . __init__ ( config , dataset , modelset , base_folder ) self . name = \"marshall random forest\" @click . command () @click . option ( \"-c\" , \"--config\" , type = str , default = os . getenv ( \"CONFIG_FILE\" ), help = \"Path to config file\" , ) @click . option ( \"--test/--no-test\" , type = bool , default = False , help = \"Flag for test\" , ) def preprocess ( config , test ): base_folder = os . getenv ( \"BASE_FOLDER\" ) _CONFIG = Config ( config , base_folder = base_folder ) preprocessed_data_config = _CONFIG [[ \"preprocessing\" , \"dataset\" , \"preprocessed\" ]] rf_config = _CONFIG [[ \"model\" , \"rf\" ]] # create folders prepare_folders ( preprocessed_data_config [ \"local\" ], base_folder = base_folder ) prepare_folders ( _CONFIG [[ \"model\" , \"rf\" , \"artifacts\" , \"model\" , \"local\" ]], base_folder = base_folder ) # load transformed data logger . debug ( f \"Loading data ...\" ) df = load_data ( preprocessed_data_config [ \"name_absolute\" ]) if test is True : df = df . sample ( 1000 ) # model logger . debug ( f \"Prepare modelset and dataset\" ) D = DataSet ( config = rf_config , base_folder = base_folder ) M = ModelSet ( config = rf_config , base_folder = base_folder ) logger . debug ( f \"Assemble randomforest\" ) rf = RandomForest ( config = rf_config , dataset = D , modelset = M ) logger . debug ( f \"Training ...\" ) rf . train ( df ) if __name__ == \"__main__\" : preprocess ()","title":"Random Forest"},{"location":"tutorials/rideindego/transform/","text":"Transform \u00a4 The zip files from the previous step ( Extract ) are cleaned up and saved as one data file using TripDataCleansing . Pipeline import os import click import pandas as pd import simplejson as json from dotenv import load_dotenv from haferml.blend.config import Config from haferml.sync.local import prepare_folders from loguru import logger from utils.transformer import TripDataCleansing from utils.transformer import get_all_raw_files from utils.transformer import load_data load_dotenv () @click . command () @click . option ( \"-c\" , \"--config\" , type = str , default = os . getenv ( \"CONFIG_FILE\" ), help = \"Path to config file\" , ) def transform ( config ): base_folder = os . getenv ( \"BASE_FOLDER\" ) _CONFIG = Config ( config , base_folder = base_folder ) etl_trip_data_config = _CONFIG [[ \"etl\" , \"raw\" , \"trip_data\" ]] transformed_trip_data_config = _CONFIG [[ \"etl\" , \"transformed\" , \"trip_data\" ]] # create folders prepare_folders ( etl_trip_data_config [ \"local\" ], base_folder ) prepare_folders ( transformed_trip_data_config [ \"local\" ], base_folder ) # load raw data raw_data_files = get_all_raw_files ( etl_trip_data_config [ \"local_absolute\" ]) dataset = load_data ( raw_data_files ) # data cleansing logger . info ( \"Cleaning up data\" ) cleaner = TripDataCleansing ( config = _CONFIG , target_local = transformed_trip_data_config [ \"name_absolute\" ], ) cleaner . run ( dataset ) logger . info ( \"Saved clean data: {} \" . format ( cleaner . target_local )) if __name__ == \"__main__\" : transform () TripDataCleansing import datetime import os import numpy as np import pandas as pd from loguru import logger from haferml.preprocess.pipeline import BasePreProcessor , attributes class TripDataCleansing ( BasePreProcessor ): \"\"\"Load, transform, and Dump trip data\"\"\" def __init__ ( self , config , ** params ): super ( TripDataCleansing , self ) . __init__ ( config = config , ** params ) @attributes ( order = 1 ) def _datetime_transformations ( self , dataframe ): \"\"\"Standardize datetime formats\"\"\" # extract date from datetime strings # they have different formats for dates so it is easier to # use pandas self . dataframe = dataframe self . dataframe [ \"date\" ] = self . dataframe . start_time . apply ( lambda x : x . split ( \" \" )[ 0 ] if x else None ) self . dataframe [ \"date\" ] = pd . to_datetime ( self . dataframe . date ) # extract hour of the day # there exists different time formats self . dataframe [ \"hour\" ] = self . dataframe . start_time . apply ( lambda x : int ( float ( x . split ( \" \" )[ - 1 ] . split ( \":\" )[ 0 ])) ) # get weekday self . dataframe [ \"weekday\" ] = self . dataframe . date . apply ( lambda x : x . weekday ()) # get month self . dataframe [ \"month\" ] = self . dataframe . date . apply ( lambda x : x . month ) @attributes ( order = 2 ) def _duration_normalization ( self , dataframe ): \"\"\"Duration was recorded as seconds before 2017-04-01. Here we will normalized durations to minutes \"\"\" df_all_before_2017q1 = self . dataframe . loc [ self . dataframe . date < pd . to_datetime ( datetime . date ( 2017 , 4 , 1 )) ] df_all_after_2017q1 = self . dataframe . loc [ self . dataframe . date >= pd . to_datetime ( datetime . date ( 2017 , 4 , 1 )) ] df_all_before_2017q1 [ \"duration\" ] = df_all_before_2017q1 . duration / 60 self . dataframe = pd . concat ([ df_all_before_2017q1 , df_all_after_2017q1 ]) @attributes ( order = 3 ) def _backfill_bike_types ( self , dataframe ): \"\"\"Bike types did not exist until q3 of 2018 because they only had standard before this. \"\"\" self . dataframe [ \"bike_type\" ] = self . dataframe . bike_type . fillna ( \"standard\" ) @attributes ( order = 4 ) def _fill_station_id ( self , dataframe ): \"\"\"start_station_id has null values fillna with 0 for the station id \"\"\" self . dataframe [ \"start_station_id\" ] . fillna ( 0 , inplace = True ) @attributes ( order = 5 ) def _normalize_coordinates ( self , dataframe ): \"\"\"Bike coordinates have diverging types: str or float, normalizing to float\"\"\" def convert_to_float ( data ): try : return float ( data ) except Exception : logger . debug ( f \"Can not convert { data } \" ) return np . nan self . dataframe [ \"start_lat\" ] = self . dataframe . start_lat . apply ( convert_to_float ) self . dataframe [ \"start_lon\" ] = self . dataframe . start_lon . apply ( convert_to_float ) self . dataframe [ \"end_lat\" ] = self . dataframe . end_lat . apply ( convert_to_float ) self . dataframe [ \"end_lon\" ] = self . dataframe . end_lon . apply ( convert_to_float ) @attributes ( order = 6 ) def _normalized_bike_id ( self , dataframe ): \"\"\" _normalized_bike_id bike_id can be str or int or float not all ids can be converted to int so we will use str \"\"\" self . dataframe [ \"bike_id\" ] = self . dataframe . bike_id . apply ( str ) @attributes ( order = 7 ) def _save_all_trip_data ( self , dataframe ): \"\"\"Dump all trip data to the destination define in config\"\"\" logger . debug ( self . dataframe . sample ( 10 )) self . target_local = self . params [ \"target_local\" ] try : if self . target_local . endswith ( \".parquet\" ): self . dataframe . to_parquet ( self . target_local , index = False ) elif self . target_local . endswith ( \".csv\" ): self . dataframe . to_csv ( self . target_local , index = False ) else : raise ValueError ( f \"Specified target_local is not valid (should be .csv or .parquet): { self . target_local } \" ) # TODO: should be more specific about the exceptions except Exception as ee : raise Exception ( f \"Could not save data to { self . target_local } \" )","title":"Transform"},{"location":"tutorials/rideindego/transform/#transform","text":"The zip files from the previous step ( Extract ) are cleaned up and saved as one data file using TripDataCleansing . Pipeline import os import click import pandas as pd import simplejson as json from dotenv import load_dotenv from haferml.blend.config import Config from haferml.sync.local import prepare_folders from loguru import logger from utils.transformer import TripDataCleansing from utils.transformer import get_all_raw_files from utils.transformer import load_data load_dotenv () @click . command () @click . option ( \"-c\" , \"--config\" , type = str , default = os . getenv ( \"CONFIG_FILE\" ), help = \"Path to config file\" , ) def transform ( config ): base_folder = os . getenv ( \"BASE_FOLDER\" ) _CONFIG = Config ( config , base_folder = base_folder ) etl_trip_data_config = _CONFIG [[ \"etl\" , \"raw\" , \"trip_data\" ]] transformed_trip_data_config = _CONFIG [[ \"etl\" , \"transformed\" , \"trip_data\" ]] # create folders prepare_folders ( etl_trip_data_config [ \"local\" ], base_folder ) prepare_folders ( transformed_trip_data_config [ \"local\" ], base_folder ) # load raw data raw_data_files = get_all_raw_files ( etl_trip_data_config [ \"local_absolute\" ]) dataset = load_data ( raw_data_files ) # data cleansing logger . info ( \"Cleaning up data\" ) cleaner = TripDataCleansing ( config = _CONFIG , target_local = transformed_trip_data_config [ \"name_absolute\" ], ) cleaner . run ( dataset ) logger . info ( \"Saved clean data: {} \" . format ( cleaner . target_local )) if __name__ == \"__main__\" : transform () TripDataCleansing import datetime import os import numpy as np import pandas as pd from loguru import logger from haferml.preprocess.pipeline import BasePreProcessor , attributes class TripDataCleansing ( BasePreProcessor ): \"\"\"Load, transform, and Dump trip data\"\"\" def __init__ ( self , config , ** params ): super ( TripDataCleansing , self ) . __init__ ( config = config , ** params ) @attributes ( order = 1 ) def _datetime_transformations ( self , dataframe ): \"\"\"Standardize datetime formats\"\"\" # extract date from datetime strings # they have different formats for dates so it is easier to # use pandas self . dataframe = dataframe self . dataframe [ \"date\" ] = self . dataframe . start_time . apply ( lambda x : x . split ( \" \" )[ 0 ] if x else None ) self . dataframe [ \"date\" ] = pd . to_datetime ( self . dataframe . date ) # extract hour of the day # there exists different time formats self . dataframe [ \"hour\" ] = self . dataframe . start_time . apply ( lambda x : int ( float ( x . split ( \" \" )[ - 1 ] . split ( \":\" )[ 0 ])) ) # get weekday self . dataframe [ \"weekday\" ] = self . dataframe . date . apply ( lambda x : x . weekday ()) # get month self . dataframe [ \"month\" ] = self . dataframe . date . apply ( lambda x : x . month ) @attributes ( order = 2 ) def _duration_normalization ( self , dataframe ): \"\"\"Duration was recorded as seconds before 2017-04-01. Here we will normalized durations to minutes \"\"\" df_all_before_2017q1 = self . dataframe . loc [ self . dataframe . date < pd . to_datetime ( datetime . date ( 2017 , 4 , 1 )) ] df_all_after_2017q1 = self . dataframe . loc [ self . dataframe . date >= pd . to_datetime ( datetime . date ( 2017 , 4 , 1 )) ] df_all_before_2017q1 [ \"duration\" ] = df_all_before_2017q1 . duration / 60 self . dataframe = pd . concat ([ df_all_before_2017q1 , df_all_after_2017q1 ]) @attributes ( order = 3 ) def _backfill_bike_types ( self , dataframe ): \"\"\"Bike types did not exist until q3 of 2018 because they only had standard before this. \"\"\" self . dataframe [ \"bike_type\" ] = self . dataframe . bike_type . fillna ( \"standard\" ) @attributes ( order = 4 ) def _fill_station_id ( self , dataframe ): \"\"\"start_station_id has null values fillna with 0 for the station id \"\"\" self . dataframe [ \"start_station_id\" ] . fillna ( 0 , inplace = True ) @attributes ( order = 5 ) def _normalize_coordinates ( self , dataframe ): \"\"\"Bike coordinates have diverging types: str or float, normalizing to float\"\"\" def convert_to_float ( data ): try : return float ( data ) except Exception : logger . debug ( f \"Can not convert { data } \" ) return np . nan self . dataframe [ \"start_lat\" ] = self . dataframe . start_lat . apply ( convert_to_float ) self . dataframe [ \"start_lon\" ] = self . dataframe . start_lon . apply ( convert_to_float ) self . dataframe [ \"end_lat\" ] = self . dataframe . end_lat . apply ( convert_to_float ) self . dataframe [ \"end_lon\" ] = self . dataframe . end_lon . apply ( convert_to_float ) @attributes ( order = 6 ) def _normalized_bike_id ( self , dataframe ): \"\"\" _normalized_bike_id bike_id can be str or int or float not all ids can be converted to int so we will use str \"\"\" self . dataframe [ \"bike_id\" ] = self . dataframe . bike_id . apply ( str ) @attributes ( order = 7 ) def _save_all_trip_data ( self , dataframe ): \"\"\"Dump all trip data to the destination define in config\"\"\" logger . debug ( self . dataframe . sample ( 10 )) self . target_local = self . params [ \"target_local\" ] try : if self . target_local . endswith ( \".parquet\" ): self . dataframe . to_parquet ( self . target_local , index = False ) elif self . target_local . endswith ( \".csv\" ): self . dataframe . to_csv ( self . target_local , index = False ) else : raise ValueError ( f \"Specified target_local is not valid (should be .csv or .parquet): { self . target_local } \" ) # TODO: should be more specific about the exceptions except Exception as ee : raise Exception ( f \"Could not save data to { self . target_local } \" )","title":"Transform"}]}